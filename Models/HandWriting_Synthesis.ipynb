{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISwNp1BCLz7L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM, Dense,Attention ,Concatenate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Uploading google drive on colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmANewmhmt6X",
        "outputId": "a427f8fb-a0d4-4ff3-e559-a0eac0f10fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pBRO9lGNjy6"
      },
      "outputs": [],
      "source": [
        "#Accessing pre-processed datasets fron files\n",
        "x_len=np.load('/content/drive/MyDrive/writemate data/x_len.npy')\n",
        "x=np.load('/content/drive/MyDrive/writemate data/x.npy')\n",
        "c_len=np.load('/content/drive/MyDrive/writemate data/c_len.npy')\n",
        "c=np.load('/content/drive/MyDrive/writemate data/c.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTKJV46IOrLf"
      },
      "outputs": [],
      "source": [
        "#converting character sequences to one-hot\n",
        "c_hot=tf.one_hot(c,73)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping the data to be inputted\n",
        "y=x[:,1:,:]\n",
        "x=x[:,:1199,:]"
      ],
      "metadata": {
        "id": "U2AGQuXpXI6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The coordinates of y are one step ahead of x\n",
        "print(x[100][121])\n",
        "print(y[100][120])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jc1wr-lZE6-",
        "outputId": "eea18b0b-806f-4859-ba4a-c0bcc7f2b58f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.30109042 0.43289304 0.        ]\n",
            "[0.30109042 0.43289304 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E40sQXtQL3vn"
      },
      "outputs": [],
      "source": [
        "# separating the mdn parameters\n",
        "def parse_parameters(z, eps=1e-8, sigma_eps=1e-4):\n",
        "        output_mixture_components=20\n",
        "        pis, sigmas, rhos, mus, es = tf.split(\n",
        "            z,\n",
        "            [\n",
        "                1*output_mixture_components,\n",
        "                2*output_mixture_components,\n",
        "                1*output_mixture_components,\n",
        "                2*output_mixture_components,\n",
        "                1\n",
        "            ],\n",
        "            axis=-1\n",
        "        )\n",
        "        pis = tf.keras.activations.softmax(pis, axis=-1)\n",
        "        sigmas = tf.clip_by_value(tf.math.exp(sigmas), sigma_eps, np.inf)\n",
        "        rhos = tf.clip_by_value(tf.keras.activations.tanh(rhos), eps - 1.0, 1.0 - eps)\n",
        "        es = tf.clip_by_value(tf.keras.activations.sigmoid(es), eps, 1.0 - eps)\n",
        "        return pis, mus, sigmas, rhos, es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKUON2VnL8ia"
      },
      "outputs": [],
      "source": [
        "#defining and carrying out loss function\n",
        "def loss( y, lengths, pis, mus, sigmas, rho, es, eps=1e-8):\n",
        "         sigma_1, sigma_2 = tf.split(sigmas, 2, axis=2)\n",
        "         y_1, y_2, y_3 = tf.split(y, 3, axis=2)\n",
        "         mu_1, mu_2 = tf.split(mus, 2, axis=2)\n",
        "\n",
        "         norm = 1.0 / (2*np.pi*sigma_1*sigma_2 * tf.sqrt(1 - tf.square(rho)))\n",
        "         Z = tf.square((y_1 - mu_1) / (sigma_1)) + tf.square((y_2 - mu_2) / (sigma_2)) - 2*rho*(y_1 - mu_1)*(y_2 - mu_2) / (sigma_1*sigma_2)\n",
        "\n",
        "         exp = -1.0*Z / (2*(1 - tf.square(rho)))\n",
        "         gaussian_likelihoods = tf.exp(exp) * norm\n",
        "         gmm_likelihood = tf.reduce_sum(pis * gaussian_likelihoods, 2)\n",
        "         gmm_likelihood = tf.clip_by_value(gmm_likelihood, eps, np.inf)\n",
        "\n",
        "         bernoulli_likelihood = tf.squeeze(tf.where(tf.equal(tf.ones_like(y_3), y_3), es, 1 - es))\n",
        "\n",
        "         nll = -(tf.math.log(gmm_likelihood) + tf.math.log(bernoulli_likelihood))\n",
        "         sequence_mask = tf.logical_and(\n",
        "            tf.sequence_mask(lengths, maxlen=tf.shape(y)[1]),\n",
        "            tf.logical_not(tf.math.is_nan(nll)),\n",
        "         )\n",
        "         nll = tf.where(sequence_mask, nll, tf.zeros_like(nll))\n",
        "         num_valid = tf.reduce_sum(tf.cast(sequence_mask, tf.float32), axis=1)\n",
        "\n",
        "         sequence_loss = tf.reduce_sum(nll, axis=1) / tf.maximum(num_valid, 1.0)\n",
        "        # element_loss = tf.reduce_sum(nll) / tf.maximum(tf.reduce_sum(num_valid), 1.0)\n",
        "         return sequence_loss #, element_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM and attention layer combined\n",
        "class LSTM_Attention(tf.keras.Model):\n",
        "  def __init__(self, sent_max_len, batch_size,\n",
        "                 hidden_size,  win_in_size, win_out_size\n",
        "                 ):\n",
        "    super(LSTM_Attention, self).__init__()\n",
        "    self.sent_max_len = sent_max_len\n",
        "    # self.vars_per_fuct = vars_per_fuct\n",
        "    self.lstm1 = tf.keras.layers.LSTM(400, return_sequences=True)\n",
        "    self.softwindow = tf.keras.layers.Dense(win_out_size)\n",
        "  def call(self, strks, onehots, sents_m, w_prev, k_prev, prev):\n",
        "        timesteps = strks.shape[1]\n",
        "       # sent_real_len = tf.reduce_sum(sents_m, axis=1)\n",
        "        h1_list, wt_list = [], []\n",
        "\n",
        "        for t in range(timesteps):\n",
        "            # concat the stroke feature and sentence feature\n",
        "            input_t = tf.concat([tf.squeeze(strks[:, t, :]), w_prev], axis=1)\n",
        "            # first LSTM layer\n",
        "          #  prev=tf.reshape(prev,(32,76,1))\n",
        "            prev = self.lstm1(input_t, prev)\n",
        "            h1_t = prev[0]\n",
        "            # nn.utils.clip_grad_value_(h1_t, 10)  # gradient clip for LSTM\n",
        "            h1_list.append(h1_t)\n",
        "\n",
        "            # >>> attention mechanisim, formula (46) ~ (57)\n",
        "\n",
        "            p = self.softwindow(h1_t)\n",
        "            a, b, k = tf.split(p, num_or_size_splits=3, axis=1)\n",
        "            a, b, k = tf.exp(a), tf.exp(b), k_prev + tf.exp(k)\n",
        "            # >>>> compute the \"dist\" between current pos and all positions\n",
        "\n",
        "            u = tf.constant(np.arange(self.sent_max_len + 1), dtype=tf.float32)\n",
        "            u = tf.expand_dims(u, axis=0)\n",
        "            # a, b, k for each pos of input sent and guassian functions\n",
        "\n",
        "            pos = tf.expand_dims(k, axis=2) - u\n",
        "            gravity = -1 * tf.expand_dims(b, axis=2) * (pos**2)\n",
        "            phi = tf.reduce_sum(tf.expand_dims(a, axis=2) * tf.exp(gravity), axis=1)\n",
        "            except_last = tf.expand_dims(tf.slice(phi, [0, 0, 0], [-1, self.sent_max_len, 1]), axis=2)\n",
        "            w_t = tf.reduce_sum(except_last * onehots, axis=1)\n",
        "            # print('w_t shape:', w_t.shape)\n",
        "            # exit(0)\n",
        "\n",
        "            wt_list.append(w_t)\n",
        "            # >>>\n",
        "\n",
        "            # update parameters for next timestep\n",
        "            k_prev = k\n",
        "            w_prev = w_t\n",
        "\n",
        "        # collection the hidden state from LSTM1 for LSTM2\n",
        "        hid1 = tf.stack(h1_list, axis=1)  # (batch, timesteps, hidden_size)\n",
        "        win_vec = tf.squeeze(tf.stack(wt_list, axis=1), axis=2)  # (batch, timesteps, len(alphabet))\n",
        "        return hid1, prev, win_vec, w_prev, k_prev, phi"
      ],
      "metadata": {
        "id": "rFfk4MbhFPWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "w_prev,k_prev,prev should be initalised with zeros of shape (batch_size,hidden_dim)"
      ],
      "metadata": {
        "id": "dypN-UPBIGbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The handwriting synthesis model\n",
        "class HandwritingSynthesis(tf.keras.Model):\n",
        "    def __init__(self, sent_max_len, batch_size, hidden_size=400):\n",
        "        super(HandwritingSynthesis, self).__init__()\n",
        "        # self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = 3\n",
        "        strk_dim, sent_dim = (3, 75)\n",
        "        lstm1_in_size = strk_dim + sent_dim\n",
        "        win_in_size = hidden_size\n",
        "        win_out_size = 10 * 3 # 3K\n",
        "        mdn_out_size = 1 + ((1 + 1 + 2 + 2) * 20)\n",
        "\n",
        "        self.lstm1 = LSTM_Attention(sent_max_len, lstm1_in_size, hidden_size, win_in_size, win_out_size)\n",
        "        self.lstm2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True) # batch_first=True\n",
        "        self.lstm3 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True) # batch_first=True\n",
        "        self.mdn = tf.keras.layers.Dense(121, activation='relu')\n",
        "        self.loss = CustomMSE()\n",
        "\n",
        "        # self.tanh = tf.keras.layers.Activation('tanh')\n",
        "    def call(self,x,x_len,c_hot,c_len,w_prev,k_prev,prev1,prev2,prev3,batch_size,win_out_size,bias=0.):\n",
        "\n",
        "        timesteps = x.shape[1]\n",
        "        sent_len = c_len.shape[0]\n",
        "        # LSTM 1 ( with attention mechanism )\n",
        "        hid1, prev1, win_vec, w_prev, k_prev, phi_prev = self.lstm1(\n",
        "            x, c_hot, c_len,w_prev,k_prev, prev1)\n",
        "\n",
        "        # LSTM 2\n",
        "        # Skip connection, LSTM1's output and window vector\n",
        "        lstm2_input = tf.concat([x, hid1, win_vec], axis=-1)\n",
        "        hid2, _, _ = self.lstm2(lstm2_input, initial_state=[prev2, prev2])\n",
        "\n",
        "        # LSTM 3\n",
        "        lstm3_input = tf.concat([x, hid2, win_vec], axis=-1)\n",
        "        hid3, _, _ = self.lstm3(lstm3_input, initial_state=[prev3, prev3])\n",
        "\n",
        "        # Mixture Gaussian Network\n",
        "        lstm_output = tf.concat([hid1, hid2, hid3], axis=-1)\n",
        "        params = self.mdn(lstm_output)\n",
        "\n",
        "        pis, mus, sigmas, rhos, es = parse_parameters(params, eps=1e-8, sigma_eps=1e-4)\n",
        "        loss =  self.loss(y ,x_len, pis, mus, sigmas, rhos, es, eps=1e-8)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "O13jCFjAhTP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "def train_conditional_model(input_data):\n",
        " batch_size = 32\n",
        " num_of_batches = 11911 // batch_size\n",
        " K = 10\n",
        " hidden_size = 400\n",
        "\n",
        "# Define the model\n",
        " model = HandwritingSynthesis(75, batch_size=batch_size, hidden_size=hidden_size)\n",
        "\n",
        "# Initialize initial values for k_prev, h1, c1, h2, c2, h3, c3\n",
        " k_prev = tf.zeros((batch_size, K), dtype=tf.float32)\n",
        " h1 = c1 = tf.zeros((batch_size, hidden_size), dtype=tf.float32)\n",
        " h2 = c2 = tf.zeros((1, batch_size, hidden_size), dtype=tf.float32)\n",
        " h3 = c3 = tf.zeros((1, batch_size, hidden_size), dtype=tf.float32)\n",
        "\n",
        "# Define optimizer (replace with your optimizer)\n",
        " optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        " num_epochs=10\n",
        "# Training loop\n",
        " for epoch in range(num_epochs):\n",
        "    ct_loss = 0\n",
        "    for i in range(0, batch_size * num_of_batches, batch_size):\n",
        "        strks, strks_m, onehots,sent_m = x[i:i + batch_size], x_len[i:i + batch_size], c_hot[i:i + batch_size],c_len[i:i+batch_size]\n",
        "        target = y\n",
        "        w_prev = onehots[i:i+batch_size,0,:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = model(strks,strks_m,onehots,sent_m,w_prev,k_prev,(h1, c1), (h2, c2), (h3, c2),32,30)\n",
        "            ct_loss += loss\n",
        "#x, strks_m, onehots, sents_m, w_prev, k_prev,(h1, c1), (h2, c2), (h3, c2)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "TuO4UiFVcV0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "train_conditional_model(input_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "fEPOjc4fZDs4",
        "outputId": "7e3f0c45-6a52-4285-d392-54018824fbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-2d963bb79bd5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_conditional_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-61-111af151f100>\u001b[0m in \u001b[0;36mtrain_conditional_model\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mw_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrks_m\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0monehots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_m\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mct_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#x, strks_m, onehots, sents_m, w_prev, k_prev,(h1, c1), (h2, c2), (h3, c2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-02facc98a64d>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, x_len, c_hot, c_len, w_prev, k_prev, prev1, prev2, prev3, batch_size, win_out_size, bias)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msent_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# LSTM 1 ( with attention mechanism )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         hid1, prev1, win_vec, w_prev, k_prev, phi_prev = self.lstm1(\n\u001b[0m\u001b[1;32m     27\u001b[0m             x, c_hot, c_len,w_prev,k_prev, prev1)\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-554c7b3d6d7b>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, strks, onehots, sents_m, w_prev, k_prev, prev)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# first LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0;31m#  prev=tf.reshape(prev,(32,76,1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mh1_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# nn.utils.clip_grad_value_(h1_t, 10)  # gradient clip for LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'lstm__attention_19' (type LSTM_Attention).\n\nInput 0 of layer \"lstm_57\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (32, 76)\n\nCall arguments received by layer 'lstm__attention_19' (type LSTM_Attention):\n  • strks=tf.Tensor(shape=(32, 1199, 3), dtype=float32)\n  • onehots=tf.Tensor(shape=(32, 75, 73), dtype=float32)\n  • sents_m=array([35, 35, 37, 37, 23, 35, 42, 39, 43, 39,  9, 29, 34, 34, 36, 39,  9,\n       31, 30, 33, 30, 33, 34, 25, 18, 26, 23, 22, 27, 29, 34, 27],\n      dtype=int8)\n  • w_prev=tf.Tensor(shape=(32, 73), dtype=float32)\n  • k_prev=tf.Tensor(shape=(32, 10), dtype=float32)\n  • prev=('tf.Tensor(shape=(32, 400), dtype=float32)', 'tf.Tensor(shape=(32, 400), dtype=float32)')"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}