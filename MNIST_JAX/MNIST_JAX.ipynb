{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYNrj1sx9Fi6",
        "outputId": "8889b942-276e-4d8e-dc71-17c62be571d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "from jax import ops\n",
        "import struct\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "from array import array\n",
        "from os.path  import join\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#allocatig data\n",
        "x,y = mnist['data'],mnist['target']\n",
        "\n",
        "x_train,x_test = x[:60000], x[60000:]\n",
        "y_train,y_test = y[:60000], y[60000:]\n",
        "\n",
        "x_train =jnp.array(x_train, dtype=jnp.int32)\n",
        "x_test = jnp.array(x_test,dtype = jnp.int32)\n",
        "y_train = jnp.array(y_train,dtype = jnp.int32)\n",
        "y_test = jnp.array(y_test,dtype=jnp.int32)"
      ],
      "metadata": {
        "id": "1kSIy04sEVCk"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading data on gpu\n",
        "if jax.devices(\"gpu\"):\n",
        "    gpu_device = jax.devices(\"gpu\")[0]\n",
        "\n",
        "    x_train = jax.device_put(x_train, device=gpu_device)\n",
        "    y_train = jax.device_put(y_train, device=gpu_device)\n",
        "    x_test = jax.device_put(x_test, device=gpu_device)\n",
        "    y_test = jax.device_put(y_test, device=gpu_device)\n",
        "else:\n",
        "    print(\"No GPU devices found.\")\n"
      ],
      "metadata": {
        "id": "biqJJqRPlU9U"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = jnp.transpose(jnp.reshape(x_train, (60000, -1)))\n",
        "x_test = jnp.transpose(jnp.reshape(x_test, (10000, -1)))\n",
        "y_train = jnp.reshape(y_train, (1, 60000))\n",
        "y_test = jnp.reshape(y_test, (1, 10000))"
      ],
      "metadata": {
        "id": "c4cYyB0hFwe6"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ormalising data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "metadata": {
        "id": "T-JXgb23G_09"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(x):\n",
        "  #inputs linear units\n",
        "  #outputs non-linear units\n",
        "  #outputs positive number ass it is or outputs 0\n",
        "  y = jnp.maximum(0, x)\n",
        "  return y\n",
        "\n",
        "def softmax(x):\n",
        "  #Used when more than 2 class-classification is involved in this case 10\n",
        "\n",
        "  exp_x = jnp.exp(x) #coverts all values into positive; amplifies diferences in values\n",
        "  sum_exp_x = jnp.sum(exp_x, axis=0, keepdims=True) #axis =0 calculated along rows, keepdims = true dimension same as exp_x\n",
        "  y = exp_x / sum_exp_x\n",
        "  return y\n"
      ],
      "metadata": {
        "id": "_y8eYmeNHemi"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_params():\n",
        "  #geerating matrix of random values for Weights and matrix of random values for biases\n",
        "  key = random.PRNGKey(0)\n",
        "  W1 = random.normal(key, (36, 784))*0.01\n",
        "  W2 = random.normal(key, (10, 36))*0.01\n",
        "  b1 = jnp.zeros((36, 1))\n",
        "  b2 = jnp.zeros((10, 1))\n",
        "  return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "8y_dU4Xz_6fx"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(W1,b1,W2,b2,X) :\n",
        "  #inputs W1 , b1 - weight matrix and bias matrix for first layer\n",
        "  #inputs W2,  b2 - weight matrix and bias matrix for second layer\n",
        "  #inputs X - activation function of initial layer\n",
        "  Z1 = jnp.dot(W1, X) + b1\n",
        "  A1 = ReLU(Z1)\n",
        "  Z2 = jnp.dot(W2,A1) + b2\n",
        "  A2 = softmax(Z2)\n",
        "  return Z1, A1, Z2, A2"
      ],
      "metadata": {
        "id": "U3_6VYaPIpWe"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cost(A2, Y):\n",
        "  #inputs A2- output of forward propogation\n",
        "  #inputs Y- list of correct predictions of model\n",
        "  #claculated using BINARY CROSS ENTROPY LOSS\n",
        "  # Elementwise multiplication of Y and log(A2) : if Y is 1, then A2 must be close to ! or the function will give large value of cost\n",
        "  #Similarly (1-Y)*log(1-A2) is for when Y is 0\n",
        "  #Dividing len(Y)- to find average of the function : len(Y)- number of elements in list Y\n",
        "  cost = -jnp.sum(Y * jnp.log(A2) + (1 - Y) * jnp.log(1 - A2)) / len(Y)\n",
        "  return cost\n"
      ],
      "metadata": {
        "id": "vMcD_3f8KxFP"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(Y):\n",
        "  #Y- array of class lael containing correct labels\n",
        "  #computing number of samples in dataset\n",
        "  Y_size = jnp.size(Y)\n",
        "  #calculating number of unique class in dataset\n",
        "  Y_max = jnp.max(Y)\n",
        "  #making a matrix of zeros with number of columns - number of classes; number of rows- number of dataset\n",
        "  one_hot_Y = jnp.zeros((Y_size, Y_max + 1), dtype=jnp.int32)\n",
        "  #sets 0 to 1 at row-jnp.arange(Y_size); column - Y\n",
        "  one_hot_Y=one_hot_Y.at[jnp.arange(Y_size), Y].set(1)\n",
        "  one_hot_Y = jnp.transpose(one_hot_Y)\n",
        "  return one_hot_Y\n"
      ],
      "metadata": {
        "id": "rJTr4KflNN1Z"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_prop(Z1, A1, Z2, A2, W2, X, Y):\n",
        "  m = Y.size\n",
        "  one_hot_Y = one_hot(Y)\n",
        "  #derivative of loss\n",
        "  dZ2 = A2 - one_hot_Y\n",
        "  #calculatig derivatives\n",
        "  dW2 = 1/m * jnp.dot(dZ2,jnp.transpose(A1))\n",
        "  db2 = 1/m * jnp.sum(dZ2, axis =1, keepdims = True)\n",
        "  dZ1 = jnp.dot(jnp.transpose(W2), dZ2) * deriv_ReLU(Z1)\n",
        "  dW1 = 1/m * jnp.dot(dZ1,jnp.transpose(X))\n",
        "  db1 = 1/m * jnp.sum(dZ1,axis = 1, keepdims = True)\n",
        "  return dW1, db1, dW2, db2"
      ],
      "metadata": {
        "id": "t32WXLrRP2AK"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deriv_ReLU(z):\n",
        "  #taking derivative for back-prop\n",
        "  return z>0"
      ],
      "metadata": {
        "id": "02v1tfLEQqOk"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "#updating params to pass them into forward prop to increase accuracy\n",
        "  W1 = W1 - alpha*dW1\n",
        "  W2 = W2 - alpha*dW2\n",
        "  b1 = b1 - alpha*db1\n",
        "  b2 = b2 - alpha*db2\n",
        "  return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "KIppg8XDQ_jn"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_predictions(A2):\n",
        "  #calculates maximum value along row of each column\n",
        "  y= jnp.argmax(A2, axis=0)\n",
        "  return y"
      ],
      "metadata": {
        "id": "8hwvKexMRXQe"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(predictions, Y):\n",
        "  #returns true if predictions matches the correct predictions of dataset; sums the total number of true we get\n",
        "  num = jnp.sum(predictions == Y)\n",
        "  #ratio of total number of trues and total number of predictions\n",
        "  accuracy = (num / jnp.size(Y))*100\n",
        "  print(predictions, Y)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "0406bYaCScBi"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def result(X, Y, iterations, alpha):\n",
        "  #initialising weights and biases\n",
        "  W1, b1, W2, b2 = initialize_params()\n",
        "\n",
        "  for i in range(0, iterations):\n",
        "    #initialisind a list to contain cost values\n",
        "    costs = []\n",
        "    #calculating forward propogation\n",
        "    Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "    #calculatig cost\n",
        "    cost = calculate_cost(A2, one_hot(Y))\n",
        "    #ackward propogation\n",
        "    dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W2, X, Y)\n",
        "    #updating parameters\n",
        "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "    if i%100 == 0:\n",
        "      print(\"At iteration \" + str(i) + \"the cost is \" + str(cost))\n",
        "    if i % 100 == 0 or i == iterations:\n",
        "      print(\"Iterations: \" + str(i))\n",
        "      print(\"Accuracy  :\" + str(calculate_accuracy(calculate_predictions(A2), Y)))\n",
        "  return cost"
      ],
      "metadata": {
        "id": "lNyg8taUTGA5"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost = result(x_train, y_train, 6000, 0.2)"
      ],
      "metadata": {
        "id": "wA9ebRMFVQ2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff3ff63-bbec-470b-d8f7-3ca6cb743e7e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At iteration 0the cost is 19504.91\n",
            "Iterations: 0\n",
            "[8 3 8 ... 8 3 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :7.545\n",
            "At iteration 100the cost is 5699.3877\n",
            "Iterations: 100\n",
            "[3 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :85.113335\n",
            "At iteration 200the cost is 3870.2148\n",
            "Iterations: 200\n",
            "[3 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :89.405\n",
            "At iteration 300the cost is 3367.0032\n",
            "Iterations: 300\n",
            "[3 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :90.56\n",
            "At iteration 400the cost is 3098.3286\n",
            "Iterations: 400\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :91.31167\n",
            "At iteration 500the cost is 2914.5728\n",
            "Iterations: 500\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :91.86667\n",
            "At iteration 600the cost is 2770.5107\n",
            "Iterations: 600\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :92.24333\n",
            "At iteration 700the cost is 2646.9397\n",
            "Iterations: 700\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :92.61\n",
            "At iteration 800the cost is 2534.5496\n",
            "Iterations: 800\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :92.98334\n",
            "At iteration 900the cost is 2429.3572\n",
            "Iterations: 900\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :93.30334\n",
            "At iteration 1000the cost is 2328.9324\n",
            "Iterations: 1000\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :93.58833\n",
            "At iteration 1100the cost is 2234.555\n",
            "Iterations: 1100\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :93.90167\n",
            "At iteration 1200the cost is 2146.7878\n",
            "Iterations: 1200\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :94.18\n",
            "At iteration 1300the cost is nan\n",
            "Iterations: 1300\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :94.405\n",
            "At iteration 1400the cost is nan\n",
            "Iterations: 1400\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :94.630005\n",
            "At iteration 1500the cost is 1922.2795\n",
            "Iterations: 1500\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :94.835\n",
            "At iteration 1600the cost is nan\n",
            "Iterations: 1600\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :95.055\n",
            "At iteration 1700the cost is nan\n",
            "Iterations: 1700\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :95.19667\n",
            "At iteration 1800the cost is nan\n",
            "Iterations: 1800\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :95.348335\n",
            "At iteration 1900the cost is nan\n",
            "Iterations: 1900\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :95.488335\n",
            "At iteration 2000the cost is nan\n",
            "Iterations: 2000\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :95.595\n",
            "At iteration 2100the cost is nan\n",
            "Iterations: 2100\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :95.706665\n",
            "At iteration 2200the cost is nan\n",
            "Iterations: 2200\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :95.83833\n",
            "At iteration 2300the cost is nan\n",
            "Iterations: 2300\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :95.99167\n",
            "At iteration 2400the cost is nan\n",
            "Iterations: 2400\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.09333\n",
            "At iteration 2500the cost is nan\n",
            "Iterations: 2500\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.20333\n",
            "At iteration 2600the cost is nan\n",
            "Iterations: 2600\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.316666\n",
            "At iteration 2700the cost is nan\n",
            "Iterations: 2700\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.42834\n",
            "At iteration 2800the cost is nan\n",
            "Iterations: 2800\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.528336\n",
            "At iteration 2900the cost is nan\n",
            "Iterations: 2900\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.613335\n",
            "At iteration 3000the cost is nan\n",
            "Iterations: 3000\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.67\n",
            "At iteration 3100the cost is nan\n",
            "Iterations: 3100\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.75166\n",
            "At iteration 3200the cost is nan\n",
            "Iterations: 3200\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.848335\n",
            "At iteration 3300the cost is nan\n",
            "Iterations: 3300\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.903336\n",
            "At iteration 3400the cost is nan\n",
            "Iterations: 3400\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :96.975\n",
            "At iteration 3500the cost is nan\n",
            "Iterations: 3500\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.02167\n",
            "At iteration 3600the cost is nan\n",
            "Iterations: 3600\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.07667\n",
            "At iteration 3700the cost is nan\n",
            "Iterations: 3700\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.13\n",
            "At iteration 3800the cost is nan\n",
            "Iterations: 3800\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.17834\n",
            "At iteration 3900the cost is nan\n",
            "Iterations: 3900\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.22\n",
            "At iteration 4000the cost is nan\n",
            "Iterations: 4000\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.25834\n",
            "At iteration 4100the cost is nan\n",
            "Iterations: 4100\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.291664\n",
            "At iteration 4200the cost is nan\n",
            "Iterations: 4200\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.35167\n",
            "At iteration 4300the cost is nan\n",
            "Iterations: 4300\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.39333\n",
            "At iteration 4400the cost is nan\n",
            "Iterations: 4400\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.439995\n",
            "At iteration 4500the cost is nan\n",
            "Iterations: 4500\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.48334\n",
            "At iteration 4600the cost is nan\n",
            "Iterations: 4600\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.53333\n",
            "At iteration 4700the cost is nan\n",
            "Iterations: 4700\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.605\n",
            "At iteration 4800the cost is nan\n",
            "Iterations: 4800\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.64167\n",
            "At iteration 4900the cost is nan\n",
            "Iterations: 4900\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.67333\n",
            "At iteration 5000the cost is nan\n",
            "Iterations: 5000\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.72833\n",
            "At iteration 5100the cost is nan\n",
            "Iterations: 5100\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.79833\n",
            "At iteration 5200the cost is nan\n",
            "Iterations: 5200\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.83\n",
            "At iteration 5300the cost is nan\n",
            "Iterations: 5300\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.855\n",
            "At iteration 5400the cost is nan\n",
            "Iterations: 5400\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.87667\n",
            "At iteration 5500the cost is nan\n",
            "Iterations: 5500\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.92167\n",
            "At iteration 5600the cost is nan\n",
            "Iterations: 5600\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.94\n",
            "At iteration 5700the cost is nan\n",
            "Iterations: 5700\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.971664\n",
            "At iteration 5800the cost is nan\n",
            "Iterations: 5800\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :97.988335\n",
            "At iteration 5900the cost is nan\n",
            "Iterations: 5900\n",
            "[5 0 4 ... 5 6 8] [[5 0 4 ... 5 6 8]]\n",
            "Accuracy  :98.01667\n"
          ]
        }
      ]
    }
  ]
}