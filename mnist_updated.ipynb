{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lLJwhPpbFyqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4443af95-e428-4ea3-ee93-71ed0b5ee0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import jax.lax as lax\n",
        "from jax import random\n",
        "import struct\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = mnist['data'],mnist['target']\n",
        "\n",
        "x_train,x_test = x[:60000], x[60000:]\n",
        "y_train,y_test = y[:60000], y[60000:]\n",
        "\n",
        "x_train =jnp.array(x_train, dtype=jnp.int32)\n",
        "x_test = jnp.array(x_test,dtype = jnp.int32)\n",
        "y_train = jnp.array(y_train,dtype = jnp.int32)\n",
        "y_test = jnp.array(y_test,dtype=jnp.int32)"
      ],
      "metadata": {
        "id": "Pw-jeEPNLqj2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if jax.devices(\"gpu\"):\n",
        "    x_train = jax.device_put(x_train, device=jax.devices(\"gpu\")[0])\n",
        "    y_train = jax.device_put(y_train, device=jax.devices(\"gpu\")[0])\n",
        "    x_test = jax.device_put(x_test, device=jax.devices(\"gpu\")[0])\n",
        "    y_test = jax.device_put(y_test, device=jax.devices(\"gpu\")[0])\n",
        "else:\n",
        "    print(\"No GPU available, using CPU.\")"
      ],
      "metadata": {
        "id": "VXExd7l8Lk5p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = jnp.transpose(jnp.reshape(x_train, (60000, -1)))\n",
        "x_test = jnp.transpose(jnp.reshape(x_test, (10000, -1)))\n",
        "y_train = jnp.reshape(y_train, (1, 60000))\n",
        "y_test = jnp.reshape(y_test, (1, 10000))\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "metadata": {
        "id": "LC0Co4_HMQ9v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(Y):\n",
        "  #Y- array of class label containing correct labels\n",
        "  #computing number of samples in dataset\n",
        "  Y_size = jnp.size(Y)\n",
        "  #calculating number of unique class in dataset\n",
        "  Y_max = jnp.max(Y)\n",
        "  one_hot_Y = jnp.zeros((Y_size, Y_max + 1), dtype=jnp.int32)\n",
        "  #sets 0 to 1 at row-jnp.arange(Y_size); column - Y\n",
        "  one_hot_Y=one_hot_Y.at[jnp.arange(Y_size), Y].set(1)\n",
        "  one_hot_Y = jnp.transpose(one_hot_Y)\n",
        "  return one_hot_Y"
      ],
      "metadata": {
        "id": "dM2AuZqBMeWU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "   key = jax.random.PRNGKey(0)\n",
        "   parameters = {}\n",
        "   L = len(layer_dims)\n",
        "   for l in range(1, L):\n",
        "          weight_key, key = jax.random.split(key)\n",
        "          #parameters['W' + str(l)] =jax.random.normal(key=jax.random.PRNGKey(0), shape=(layer_dims[l], layer_dims[l - 1])) * 0.01\n",
        "          parameters['W' + str(l)] = jax.random.normal(weight_key, shape=(layer_dims[l], layer_dims[l - 1])) * jnp.sqrt(2/layer_dims[l - 1])\n",
        "          parameters['b' + str(l)] =jnp.zeros((layer_dims[l],1))\n",
        "   return parameters"
      ],
      "metadata": {
        "id": "uaB-MO9wJHKn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    Z=jnp.dot(W,A)+b\n",
        "    cache=(A,W,b)\n",
        "    return Z, cache"
      ],
      "metadata": {
        "id": "O6lH70tyMt1W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "  #s=np.maximum(0,z)\n",
        "  #cache=(z)\n",
        "  return jnp.maximum(0,z),z"
      ],
      "metadata": {
        "id": "9ccRuk22Qj6q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "   s = jnp.exp(z)/jnp.sum(jnp.exp(z), axis = 0, keepdims = True)\n",
        "   #activation_cache = (z)\n",
        "   return s, z"
      ],
      "metadata": {
        "id": "Xqa7HfHhgEHa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "\n",
        "     if activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache=relu(Z)\n",
        "     elif activation == \"softmax\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = softmax(Z)\n",
        "     cache = (linear_cache, activation_cache)\n",
        "\n",
        "     return A, cache"
      ],
      "metadata": {
        "id": "5QGGCrv9OD1d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_forward(X, parameters):\n",
        "   caches = []\n",
        "   A = X\n",
        "   L = len(parameters) //2\n",
        "   for l in range(1,L):\n",
        "     A_prev=A\n",
        "     A, cache= linear_activation_forward(A_prev, parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\"relu\")\n",
        "     caches.append(cache)\n",
        "   AL, cache= linear_activation_forward(A, parameters[\"W\"+str(L)],parameters[\"b\"+str(L)],\"softmax\")\n",
        "\n",
        "   caches.append(cache)\n",
        "\n",
        "   return AL,caches"
      ],
      "metadata": {
        "id": "AAn6eG7Rl_Uy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -jnp.sum(Y * jnp.log(AL) + (1 - Y) * jnp.log(1 - AL)) / m\n",
        "    #jnp.squeeze(cost)\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "kJpo3NuQhc21"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "   A_prev, W, b = cache\n",
        "   m = A_prev.shape[1]\n",
        "   dW=(1/m)*(jnp.dot(dZ,A_prev.T))\n",
        "   db=(1/m)*jnp.sum(dZ,axis=1,keepdims=True)\n",
        "   dA_prev=jnp.dot(W.T,dZ)\n",
        "   return dA_prev,dW,db"
      ],
      "metadata": {
        "id": "or87GOAkn4Mn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_backward(AL, Y):\n",
        "    dZ = AL- Y\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "7Y2G7ttWhzxy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dA, cache):\n",
        "    Z = cache\n",
        "    dZ = jnp.where(Z > 0, dA, 0)\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "hJ8k1lAXi7tM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "  Y = one_hot(Y)\n",
        "  grads = {}\n",
        "  L = len(caches)\n",
        "  m = AL.shape[1]\n",
        "  Y = Y.reshape(AL.shape)\n",
        "  #dAL = -Y/AL\n",
        "  #dAL=-(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "  current_cache = caches[L-1]\n",
        "  linear_cache, activation_cache =current_cache\n",
        "  dZ=softmax_backward(AL,Y)\n",
        "  dA_prev_temp, dW_temp, db_temp =linear_backward(dZ,linear_cache)\n",
        "  grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "  grads[\"dW\" + str(L)] = dW_temp\n",
        "  grads[\"db\" + str(L)] = db_temp\n",
        "  for l in reversed(range(L-1)):\n",
        "    current_cache = caches[l]\n",
        "    linear_cache, activation_cache =current_cache\n",
        "    dZ=relu_backward( dA_prev_temp,activation_cache)\n",
        "    dA_prev_temp, dW_temp, db_temp =linear_backward(dZ,linear_cache)\n",
        "    grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(l+1)] = dW_temp\n",
        "    grads[\"db\" + str(l+1)] = db_temp\n",
        "  return grads"
      ],
      "metadata": {
        "id": "Stunl980wV4m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(params, grads, learning_rate):\n",
        "  parameters = params.copy()\n",
        "  L = len(parameters) // 2\n",
        "  for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] =parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] =parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n",
        "\n",
        "        # YOUR CODE ENDS HERE\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "6VC3M-3Pzvvy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(Y_hat):\n",
        "    return jnp.argmax(Y_hat,0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    predictions = predictions.reshape(1, predictions.shape[0])\n",
        "    check = jnp.equal(Y[predictions, jnp.arange(Y.shape[1])], 1)\n",
        "    accuracy = jnp.mean(check) * 100\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "PCIpj1ltlpwu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers_dims = [784,40,20, 10]"
      ],
      "metadata": {
        "id": "249Bf9qQ3oGx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.75, num_iterations = 3000,print_cost=False):\n",
        "  #np.random.seed(1)\n",
        "  grads = {}\n",
        "  costs = []\n",
        "  m = X.shape[1]\n",
        "  #print(layers_dims)\n",
        " # cost = 2.5\n",
        "  parameters=initialize_parameters_deep(layers_dims)\n",
        "  for i in range(0, num_iterations):\n",
        "    AL, caches=L_model_forward(X, parameters)\n",
        "    grads=L_model_backward(AL, Y, caches)\n",
        "    parameters=update_parameters(parameters, grads, learning_rate)\n",
        "    cost = compute_cost(AL,Y)\n",
        "#    Y_predict = jnp.zeros(AL.shape)\n",
        "    Y_predict = jnp.zeros_like(AL)\n",
        "    indices = (jnp.argmax(AL, axis=0), jnp.arange(AL.shape[1]))\n",
        "    Y_predict = Y_predict.at[indices].set(1)\n",
        "   # Y_predict = jax.ops.index_update(Y_predict, indices, 1)\n",
        "   # Y_predict[jnp.argmax(AL, axis = 0), jnp.arange(AL.shape[1])] = 1\n",
        "   # if i%100:\n",
        "    #  learning_rate=learning_rate/(1+0.0002)\n",
        "    if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, jnp.squeeze(cost)))\n",
        "           # print(learning_rate)\n",
        "            #print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict - Y)) * 100))\n",
        "            print(\"accuracy : \" , get_accuracy(get_predictions(AL),Y))\n",
        "\n",
        "    if i % 100 == 0 or i == num_iterations:\n",
        "            costs.append(cost)\n",
        "\n",
        "  return parameters,costs, Y_predict,AL"
      ],
      "metadata": {
        "id": "zUxqJAdp0MDF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, costs, Y_predict,AL =L_layer_model(x_train,y_train, layers_dims, 0.2, 6000, True)\n",
        "#print(\"accuracy : \" , get_accuracy(get_predictions(AL),Y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gavmHWii2j40",
        "outputId": "64384e6e-d11e-471c-c914-2d11cbd562e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 103.20845031738281\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 100: 288.9808044433594\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 200: 323.9647216796875\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 300: 343.4630126953125\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 400: 358.5415344238281\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 500: inf\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 600: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 700: inf\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 800: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 900: inf\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1000: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1100: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1200: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1300: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1400: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1500: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1600: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1700: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1800: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 1900: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2000: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2100: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2200: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2300: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2400: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2500: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2600: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2700: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2800: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 2900: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3000: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3100: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3200: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3300: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3400: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3500: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3600: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3700: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3800: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 3900: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4000: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4100: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4200: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4300: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4400: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4500: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4600: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4700: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4800: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 4900: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5000: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5100: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5200: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5300: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5400: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5500: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5600: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5700: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5800: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5900: nan\n",
            "accuracy :  11.236667\n",
            "Cost after iteration 5999: nan\n",
            "accuracy :  11.236667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mz_VLNfWQjSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}