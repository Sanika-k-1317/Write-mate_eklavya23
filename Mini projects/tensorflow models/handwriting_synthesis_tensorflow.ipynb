{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9099FQ-WacRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b020467f-f643-4669-a0bf-13e51239d3be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "from collections import namedtuple,deque,defaultdict\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import pprint as pp\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfcompat\n",
        "import tensorflow.compat.v1.distributions as tfd\n",
        "import tensorflow_probability as tfp\n",
        "tfcompat.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataFrame(object):\n",
        "    \"\"\"Minimal pd.DataFrame analog for handling n-dimensional numpy matrices with additional\n",
        "    support for shuffling, batching, and train/test splitting.\n",
        "\n",
        "    Args:\n",
        "        columns: List of names corresponding to the matrices in data.\n",
        "        data: List of n-dimensional data matrices ordered in correspondence with columns.\n",
        "            All matrices must have the same leading dimension.  Data can also be fed a list of\n",
        "            instances of np.memmap, in which case RAM usage can be limited to the size of a\n",
        "            single batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, columns, data):\n",
        "        assert len(columns) == len(data), 'columns length does not match data length'\n",
        "\n",
        "        lengths = [mat.shape[0] for mat in data]\n",
        "        assert len(set(lengths)) == 1, 'all matrices in data must have same first dimension'\n",
        "\n",
        "        self.length = lengths[0]\n",
        "        self.columns = columns\n",
        "        self.data = data\n",
        "        self.dict = dict(zip(self.columns, self.data))\n",
        "        self.idx = np.arange(self.length)\n",
        "\n",
        "    def shapes(self):\n",
        "        return pd.Series(dict(zip(self.columns, [mat.shape for mat in self.data])))\n",
        "\n",
        "    def dtypes(self):\n",
        "        return pd.Series(dict(zip(self.columns, [mat.dtype for mat in self.data])))\n",
        "\n",
        "    def shuffle(self):\n",
        "        np.random.shuffle(self.idx)\n",
        "\n",
        "    def train_test_split(self, train_size, random_state=np.random.randint(1000), stratify=None):\n",
        "        train_idx, test_idx = train_test_split(\n",
        "            self.idx,\n",
        "            train_size=train_size,\n",
        "            random_state=random_state,\n",
        "            stratify=stratify\n",
        "        )\n",
        "        train_df = DataFrame(copy.copy(self.columns), [mat[train_idx] for mat in self.data])\n",
        "        test_df = DataFrame(copy.copy(self.columns), [mat[test_idx] for mat in self.data])\n",
        "        return train_df, test_df\n",
        "\n",
        "    def batch_generator(self, batch_size, shuffle=True, num_epochs=10000, allow_smaller_final_batch=False):\n",
        "        epoch_num = 0\n",
        "        while epoch_num < num_epochs:\n",
        "            if shuffle:\n",
        "                self.shuffle()\n",
        "\n",
        "            for i in range(0, self.length + 1, batch_size):\n",
        "                batch_idx = self.idx[i: i + batch_size]\n",
        "                if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n",
        "                    break\n",
        "                yield DataFrame(\n",
        "                    columns=copy.copy(self.columns),\n",
        "                    data=[mat[batch_idx].copy() for mat in self.data]\n",
        "                )\n",
        "\n",
        "            epoch_num += 1\n",
        "\n",
        "    def iterrows(self):\n",
        "        for i in self.idx:\n",
        "            yield self[i]\n",
        "\n",
        "    def mask(self, mask):\n",
        "        return DataFrame(copy.copy(self.columns), [mat[mask] for mat in self.data])\n",
        "\n",
        "    def concat(self, other_df):\n",
        "        mats = []\n",
        "        for column in self.columns:\n",
        "            mats.append(np.concatenate([self[column], other_df[column]], axis=0))\n",
        "        return DataFrame(copy.copy(self.columns), mats)\n",
        "\n",
        "    def items(self):\n",
        "        return self.dict.items()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.dict.items().__iter__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, str):\n",
        "            return self.dict[key]\n",
        "\n",
        "        elif isinstance(key, int):\n",
        "            return pd.Series(dict(zip(self.columns, [mat[self.idx[key]] for mat in self.data])))\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        assert value.shape[0] == len(self), 'matrix first dimension does not match'\n",
        "        if key not in self.columns:\n",
        "            self.columns.append(key)\n",
        "            self.data.append(value)\n",
        "        self.dict[key] = value\n"
      ],
      "metadata": {
        "id": "K1pWGw-_bRkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n",
        "                dropout=None, scope='dense-layer', reuse=False):\n",
        "    \"\"\"\n",
        "    Applies a dense layer to a 2D tensor of shape [batch_size, input_units]\n",
        "    to produce a tensor of shape [batch_size, output_units].\n",
        "    Args:\n",
        "        inputs: Tensor of shape [batch size, input_units].\n",
        "        output_units: Number of output units.\n",
        "        activation: activation function.\n",
        "        dropout: dropout keep prob.\n",
        "    Returns:\n",
        "        Tensor of shape [batch size, output_units].\n",
        "    \"\"\"\n",
        "    with tfcompat.variable_scope(scope, reuse=reuse):\n",
        "        W = tfcompat.get_variable(\n",
        "            name='weights',\n",
        "            initializer=tfcompat.keras.initializers.VarianceScaling(scale=2.0),\n",
        "            shape=[shape(inputs, -1), output_units]\n",
        "        )\n",
        "        z = tf.matmul(inputs, W)\n",
        "        if bias:\n",
        "            b = tfcompat.get_variable(\n",
        "                name='biases',\n",
        "                initializer=tfcompat.constant_initializer(),\n",
        "                shape=[output_units]\n",
        "            )\n",
        "            z = z + b\n",
        "\n",
        "        if batch_norm is not None:\n",
        "            z = tfcompat.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n",
        "\n",
        "        z = activation(z) if activation else z\n",
        "        z = tf.nn.dropout(z, rate=1 - (dropout)) if dropout is not None else z\n",
        "        return z\n",
        "\n",
        "\n",
        "def time_distributed_dense_layer(\n",
        "        inputs, output_units, bias=True, activation=None, batch_norm=None,\n",
        "        dropout=None, scope='time-distributed-dense-layer', reuse=False):\n",
        "    \"\"\"\n",
        "    Applies a shared dense layer to each timestep of a tensor of shape\n",
        "    [batch_size, max_seq_len, input_units] to produce a tensor of shape\n",
        "    [batch_size, max_seq_len, output_units].\n",
        "\n",
        "    Args:\n",
        "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
        "        output_units: Number of output units.\n",
        "        activation: activation function.\n",
        "        dropout: dropout keep prob.\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [batch size, max sequence length, output_units].\n",
        "    \"\"\"\n",
        "    with tfcompat.variable_scope(scope, reuse=reuse):\n",
        "        W = tfcompat.get_variable(\n",
        "            name='weights',\n",
        "            initializer=tfcompat.keras.initializers.VarianceScaling(scale=2.0),\n",
        "            shape=[shape(inputs, -1), output_units]\n",
        "        )\n",
        "        z = tf.einsum('ijk,kl->ijl', inputs, W)\n",
        "        if bias:\n",
        "            b = tfcompat.get_variable(\n",
        "                name='biases',\n",
        "                initializer=tfcompat.constant_initializer(),\n",
        "                shape=[output_units]\n",
        "            )\n",
        "            z = z + b\n",
        "\n",
        "        if batch_norm is not None:\n",
        "            z = tfcompat.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n",
        "\n",
        "        z = activation(z) if activation else z\n",
        "        z = tf.nn.dropout(z, rate=1 - (dropout)) if dropout is not None else z\n",
        "        return z\n",
        "\n",
        "\n",
        "def shape(tensor, dim=None):\n",
        "    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n",
        "    if dim is None:\n",
        "        return tensor.shape.as_list()\n",
        "    else:\n",
        "        return tensor.shape.as_list()[dim]\n",
        "\n",
        "\n",
        "def rank(tensor):\n",
        "    \"\"\"Get tensor rank as python list\"\"\"\n",
        "    return len(tensor.shape.as_list())"
      ],
      "metadata": {
        "id": "A9xPBbpDbaeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LSTMAttentionCellState = namedtuple(\n",
        "    'LSTMAttentionCellState',\n",
        "    ['h1', 'c1', 'h2', 'c2', 'h3', 'c3', 'alpha', 'beta', 'kappa', 'w', 'phi']\n",
        ")\n",
        "\n",
        "\n",
        "class LSTMAttentionCell(tfcompat.nn.rnn_cell.RNNCell):\n",
        "    def __init__(\n",
        "            self,\n",
        "            lstm_size,\n",
        "            num_attn_mixture_components,\n",
        "            attention_values,\n",
        "            attention_values_lengths,\n",
        "            num_output_mixture_components,\n",
        "            bias,\n",
        "            reuse=None,\n",
        "    ):\n",
        "        self.reuse = reuse\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_attn_mixture_components = num_attn_mixture_components\n",
        "        self.attention_values = attention_values\n",
        "        self.attention_values_lengths = attention_values_lengths\n",
        "        self.window_size = shape(self.attention_values, 2)\n",
        "        self.char_len = tf.shape(attention_values)[1]\n",
        "        self.batch_size = tf.shape(attention_values)[0]\n",
        "        self.num_output_mixture_components = num_output_mixture_components\n",
        "        self.output_units = 6 * self.num_output_mixture_components + 1\n",
        "        self.bias = bias\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return LSTMAttentionCellState(\n",
        "            self.lstm_size,\n",
        "            self.lstm_size,\n",
        "            self.lstm_size,\n",
        "            self.lstm_size,\n",
        "            self.lstm_size,\n",
        "            self.lstm_size,\n",
        "            self.num_attn_mixture_components,\n",
        "            self.num_attn_mixture_components,\n",
        "            self.num_attn_mixture_components,\n",
        "            self.window_size,\n",
        "            self.char_len,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self.lstm_size\n",
        "\n",
        "    def zero_state(self, batch_size, dtype):\n",
        "        return LSTMAttentionCellState(\n",
        "            tf.zeros([batch_size, self.lstm_size]),\n",
        "            tf.zeros([batch_size, self.lstm_size]),\n",
        "            tf.zeros([batch_size, self.lstm_size]),\n",
        "            tf.zeros([batch_size, self.lstm_size]),\n",
        "            tf.zeros([batch_size, self.lstm_size]),\n",
        "            tf.zeros([batch_size, self.lstm_size]),\n",
        "            tf.zeros([batch_size, self.num_attn_mixture_components]),\n",
        "            tf.zeros([batch_size, self.num_attn_mixture_components]),\n",
        "            tf.zeros([batch_size, self.num_attn_mixture_components]),\n",
        "            tf.zeros([batch_size, self.window_size]),\n",
        "            tf.zeros([batch_size, self.char_len]),\n",
        "        )\n",
        "\n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        with tfcompat.variable_scope(scope or type(self).__name__, reuse=tfcompat.AUTO_REUSE):\n",
        "            # lstm 1\n",
        "            s1_in = tf.concat([state.w, inputs], axis=1)\n",
        "            cell1 = tfcompat.nn.rnn_cell.LSTMCell(self.lstm_size)\n",
        "            s1_out, s1_state = cell1(s1_in, state=(state.c1, state.h1))\n",
        "\n",
        "            # attention\n",
        "            attention_inputs = tf.concat([state.w, inputs, s1_out], axis=1)\n",
        "            attention_params = dense_layer(attention_inputs, 3 * self.num_attn_mixture_components, scope='attention')\n",
        "            alpha, beta, kappa = tf.split(tf.nn.softplus(attention_params), 3, axis=1)\n",
        "            kappa = state.kappa + kappa / 25.0\n",
        "            beta = tf.clip_by_value(beta, .01, np.inf)\n",
        "\n",
        "            kappa_flat, alpha_flat, beta_flat = kappa, alpha, beta\n",
        "            kappa, alpha, beta = tf.expand_dims(kappa, 2), tf.expand_dims(alpha, 2), tf.expand_dims(beta, 2)\n",
        "\n",
        "            enum = tf.reshape(tf.range(self.char_len), (1, 1, self.char_len))\n",
        "            u = tf.cast(tf.tile(enum, (self.batch_size, self.num_attn_mixture_components, 1)), tf.float32)\n",
        "            phi_flat = tf.reduce_sum(alpha * tf.exp(-tf.square(kappa - u) / beta), axis=1)\n",
        "\n",
        "            phi = tf.expand_dims(phi_flat, 2)\n",
        "            sequence_mask = tf.cast(tf.sequence_mask(self.attention_values_lengths, maxlen=self.char_len), tf.float32)\n",
        "            sequence_mask = tf.expand_dims(sequence_mask, 2)\n",
        "            w = tf.reduce_sum(phi * self.attention_values * sequence_mask, axis=1)\n",
        "\n",
        "            # lstm 2\n",
        "            s2_in = tf.concat([inputs, s1_out, w], axis=1)\n",
        "            cell2 = tfcompat.nn.rnn_cell.LSTMCell(self.lstm_size)\n",
        "            s2_out, s2_state = cell2(s2_in, state=(state.c2, state.h2))\n",
        "\n",
        "            # lstm 3\n",
        "            s3_in = tf.concat([inputs, s2_out, w], axis=1)\n",
        "            cell3 = tfcompat.nn.rnn_cell.LSTMCell(self.lstm_size)\n",
        "            s3_out, s3_state = cell3(s3_in, state=(state.c3, state.h3))\n",
        "\n",
        "            new_state = LSTMAttentionCellState(\n",
        "                s1_state.h,\n",
        "                s1_state.c,\n",
        "                s2_state.h,\n",
        "                s2_state.c,\n",
        "                s3_state.h,\n",
        "                s3_state.c,\n",
        "                alpha_flat,\n",
        "                beta_flat,\n",
        "                kappa_flat,\n",
        "                w,\n",
        "                phi_flat,\n",
        "            )\n",
        "\n",
        "            return s3_out, new_state\n",
        "\n",
        "    def output_function(self, state):\n",
        "        params = dense_layer(state.h3, self.output_units, scope='gmm', reuse=tfcompat.AUTO_REUSE)\n",
        "        pis, mus, sigmas, rhos, es = self._parse_parameters(params)\n",
        "        mu1, mu2 = tf.split(mus, 2, axis=1)\n",
        "        mus = tf.stack([mu1, mu2], axis=2)\n",
        "        sigma1, sigma2 = tf.split(sigmas, 2, axis=1)\n",
        "\n",
        "        covar_matrix = [tf.square(sigma1), rhos * sigma1 * sigma2,\n",
        "                        rhos * sigma1 * sigma2, tf.square(sigma2)]\n",
        "        covar_matrix = tf.stack(covar_matrix, axis=2)\n",
        "        covar_matrix = tf.reshape(covar_matrix, (self.batch_size, self.num_output_mixture_components, 2, 2))\n",
        "\n",
        "        mvn = tfp.distributions.MultivariateNormalFullCovariance(loc=mus, covariance_matrix=covar_matrix)\n",
        "        b = tfd.Bernoulli(probs=es)\n",
        "        c = tfd.Categorical(probs=pis)\n",
        "\n",
        "        sampled_e = b.sample()\n",
        "        sampled_coords = mvn.sample()\n",
        "        sampled_idx = c.sample()\n",
        "\n",
        "        idx = tf.stack([tf.range(self.batch_size), sampled_idx], axis=1)\n",
        "        coords = tf.gather_nd(sampled_coords, idx)\n",
        "        return tf.concat([coords, tf.cast(sampled_e, tf.float32)], axis=1)\n",
        "\n",
        "    def termination_condition(self, state):\n",
        "        char_idx = tf.cast(tf.argmax(state.phi, axis=1), tf.int32)\n",
        "        final_char = char_idx >= self.attention_values_lengths - 1\n",
        "        past_final_char = char_idx >= self.attention_values_lengths\n",
        "        output = self.output_function(state)\n",
        "        es = tf.cast(output[:, 2], tf.int32)\n",
        "        is_eos = tf.equal(es, tf.experimental.numpy.ones_like(es))\n",
        "        return tf.logical_or(tf.logical_and(final_char, is_eos), past_final_char)\n",
        "\n",
        "    def _parse_parameters(self, gmm_params, eps=1e-8, sigma_eps=1e-4):\n",
        "        pis, sigmas, rhos, mus, es = tf.split(\n",
        "            gmm_params,\n",
        "            [\n",
        "                1 * self.num_output_mixture_components,\n",
        "                2 * self.num_output_mixture_components,\n",
        "                1 * self.num_output_mixture_components,\n",
        "                2 * self.num_output_mixture_components,\n",
        "                1\n",
        "            ],\n",
        "            axis=-1\n",
        "        )\n",
        "        pis = pis * (1 + tf.expand_dims(self.bias, 1))\n",
        "        sigmas = sigmas - tf.expand_dims(self.bias, 1)\n",
        "\n",
        "        pis = tf.nn.softmax(pis, axis=-1)\n",
        "        pis = tfcompat.where(pis < .01, tf.zeros_like(pis), pis)\n",
        "        sigmas = tf.clip_by_value(tf.exp(sigmas), sigma_eps, np.inf)\n",
        "        rhos = tf.clip_by_value(tf.tanh(rhos), eps - 1.0, 1.0 - eps)\n",
        "        es = tf.clip_by_value(tf.nn.sigmoid(es), eps, 1.0 - eps)\n",
        "        es = tfcompat.where(es < .01, tf.zeros_like(es), es)\n",
        "\n",
        "        return pis, mus, sigmas, rhos, es"
      ],
      "metadata": {
        "id": "3cTgmE51cGM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/model\"\n",
        "BASE_DATA_PATH = \"data\"\n",
        "\n",
        "data_path: str = os.path.join(BASE_PATH, BASE_DATA_PATH)\n",
        "processed_data_path: str = os.path.join(data_path, \"processed\")\n",
        "raw_data_path: str = os.path.join(data_path, \"raw\")\n",
        "ascii_data_path: str = os.path.join(raw_data_path, \"ascii\")\n",
        "\n",
        "checkpoint_path: str = os.path.join(BASE_PATH, \"checkpoint\")\n",
        "prediction_path: str = os.path.join(BASE_PATH, \"prediction\")\n",
        "style_path: str = os.path.join(BASE_PATH, \"style\")\n",
        "\n",
        "class BaseModel(object):\n",
        "    \"\"\"Interface containing some boilerplate code for training tensorflow models.\n",
        "\n",
        "    Subclassing models must implement self.calculate_loss(), which returns a tensor for the batch loss.\n",
        "    Code for the training loop, parameter updates, checkpointing, and inference are implemented here and\n",
        "    subclasses are mainly responsible for building the computational graph beginning with the placeholders\n",
        "    and ending with the loss tensor.\n",
        "\n",
        "    Args:\n",
        "        reader: Class with attributes train_batch_generator, val_batch_generator, and test_batch_generator\n",
        "            that yield dictionaries mapping tf.placeholder names (as strings) to batch data (numpy arrays).\n",
        "            (handwriting_synthesis.training.DataReader)\n",
        "        batch_sizes: Minibatch size.\n",
        "        learning_rates: Learning rate.\n",
        "        optimizer: 'rms' for RMSProp, 'adam' for Adam, 'sgd' for SGD\n",
        "        grad_clip: Clip gradients elementwise to have norm at most equal to grad_clip.\n",
        "        regularization_constant:  Regularization constant applied to all trainable parameters.\n",
        "        keep_prob: 1 - p, where p is the dropout probability\n",
        "        early_stopping_steps:  Number of steps to continue training after validation loss has\n",
        "            stopped decreasing.\n",
        "        warm_start_init_step:  If nonzero, model will resume training a restored model beginning\n",
        "            at warm_start_init_step.\n",
        "        num_restarts:  After validation loss plateaus, the best checkpoint will be restored and the\n",
        "            learning rate will be halved.  This process will repeat num_restarts times.\n",
        "        enable_parameter_averaging:  If true, model saves exponential weighted averages of parameters\n",
        "            to separate checkpoint file.\n",
        "        min_steps_to_checkpoint:  Model only saves after min_steps_to_checkpoint training steps\n",
        "            have passed.\n",
        "        log_interval:  Train and validation accuracies are logged every log_interval training steps.\n",
        "        loss_averaging_window:  Train/validation losses are averaged over the last loss_averaging_window\n",
        "            training steps.\n",
        "        num_validation_batches:  Number of batches to be used in validation evaluation at each step.\n",
        "        log_dir: Directory where logs are written.\n",
        "        checkpoint_dir: Directory where checkpoints are saved.\n",
        "        prediction_dir: Directory where predictions/outputs are saved.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            reader=None,\n",
        "            batch_sizes=None,\n",
        "            num_training_steps=20000,\n",
        "            learning_rates=None,\n",
        "            beta1_decays=None,\n",
        "            optimizer='adam',\n",
        "            grad_clip=5,\n",
        "            regularization_constant=0.0,\n",
        "            keep_prob=1.0,\n",
        "            patiences=None,\n",
        "            warm_start_init_step=0,\n",
        "            enable_parameter_averaging=False,\n",
        "            min_steps_to_checkpoint=100,\n",
        "            log_interval=20,\n",
        "            logging_level=logging.INFO,\n",
        "            loss_averaging_window=100,\n",
        "            validation_batch_size=64,\n",
        "            log_dir='logs',\n",
        "            checkpoint_dir=checkpoint_path,\n",
        "            prediction_dir=prediction_path\n",
        "    ):\n",
        "\n",
        "        if batch_sizes is None:\n",
        "            batch_sizes = [128]\n",
        "        if learning_rates is None:\n",
        "            learning_rates = [.01]\n",
        "        if beta1_decays is None:\n",
        "            beta1_decays = [.99]\n",
        "        if patiences is None:\n",
        "            patiences = [3000]\n",
        "\n",
        "        self.early_stopping_metric = None\n",
        "        self.batch_size = None\n",
        "        self.learning_rate = None\n",
        "        self.beta1_decay = None\n",
        "        self.early_stopping_steps = None\n",
        "        self.metrics = {}\n",
        "        self.step = None\n",
        "        self.ema = None\n",
        "        self.global_step = None\n",
        "        self.learning_rate_var = None\n",
        "        self.beta1_decay_var = None\n",
        "        self.loss = None\n",
        "        self.saver = None\n",
        "        self.saver_averaged = None\n",
        "        self.init = None\n",
        "\n",
        "        assert len(batch_sizes) == len(learning_rates) == len(patiences)\n",
        "        self.batch_sizes = batch_sizes\n",
        "        self.learning_rates = learning_rates\n",
        "        self.beta1_decays = beta1_decays\n",
        "        self.patiences = patiences\n",
        "        self.num_restarts = len(batch_sizes) - 1\n",
        "        self.restart_idx = 0\n",
        "        self.update_train_params()\n",
        "\n",
        "        self.reader = reader\n",
        "        self.num_training_steps = num_training_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.grad_clip = grad_clip\n",
        "        self.regularization_constant = regularization_constant\n",
        "        self.warm_start_init_step = warm_start_init_step\n",
        "        self.keep_prob_scalar = keep_prob\n",
        "        self.enable_parameter_averaging = enable_parameter_averaging\n",
        "        self.min_steps_to_checkpoint = min_steps_to_checkpoint\n",
        "        self.log_interval = log_interval\n",
        "        self.loss_averaging_window = loss_averaging_window\n",
        "        self.validation_batch_size = validation_batch_size\n",
        "\n",
        "        self.log_dir = log_dir\n",
        "        self.logging_level = logging_level\n",
        "        self.prediction_dir = prediction_dir\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        if self.enable_parameter_averaging:\n",
        "            self.checkpoint_dir_averaged = checkpoint_dir + '_avg'\n",
        "\n",
        "        self.init_logging(self.log_dir)\n",
        "        logging.info('\\nNew run with parameters:\\n{}'.format(pp.pformat(self.__dict__)))\n",
        "\n",
        "        self.graph = self.build_graph()\n",
        "        self.session = tfcompat.Session(graph=self.graph)\n",
        "        logging.info('Built Graph')\n",
        "\n",
        "    def update_train_params(self):\n",
        "        self.batch_size = self.batch_sizes[self.restart_idx]\n",
        "        self.learning_rate = self.learning_rates[self.restart_idx]\n",
        "        self.beta1_decay = self.beta1_decays[self.restart_idx]\n",
        "        self.early_stopping_steps = self.patiences[self.restart_idx]\n",
        "\n",
        "    def calculate_loss(self):\n",
        "        raise NotImplementedError('Subclass must implement this.')\n",
        "\n",
        "    def fit(self):\n",
        "        with self.session.as_default():\n",
        "\n",
        "            if self.warm_start_init_step:\n",
        "                self.restore(self.warm_start_init_step)\n",
        "                step = self.warm_start_init_step\n",
        "            else:\n",
        "                self.session.run(self.init)\n",
        "                step = 0\n",
        "\n",
        "            train_generator = self.reader.train_batch_generator(self.batch_size)\n",
        "            val_generator = self.reader.val_batch_generator(self.validation_batch_size)\n",
        "\n",
        "            train_loss_history = deque(maxlen=self.loss_averaging_window)\n",
        "            val_loss_history = deque(maxlen=self.loss_averaging_window)\n",
        "            train_time_history = deque(maxlen=self.loss_averaging_window)\n",
        "            val_time_history = deque(maxlen=self.loss_averaging_window)\n",
        "\n",
        "            metric_histories = {\n",
        "                metric_name: deque(maxlen=self.loss_averaging_window) for metric_name in self.metrics\n",
        "            }\n",
        "            best_validation_loss, best_validation_tstep = float('inf'), 0\n",
        "            checkpoint_created=False\n",
        "\n",
        "\n",
        "            while step < self.num_training_steps:\n",
        "\n",
        "                # validation evaluation\n",
        "                val_start = time.time()\n",
        "                val_batch_df = next(val_generator)\n",
        "                val_feed_dict = {\n",
        "                    getattr(self, placeholder_name, None): data\n",
        "                    for placeholder_name, data in val_batch_df.items() if hasattr(self, placeholder_name)\n",
        "                }\n",
        "\n",
        "                val_feed_dict.update(\n",
        "                    {self.learning_rate_var: self.learning_rate, self.beta1_decay_var: self.beta1_decay})\n",
        "                if hasattr(self, 'keep_prob'):\n",
        "                    val_feed_dict.update({self.keep_prob: 1.0})\n",
        "                if hasattr(self, 'is_training'):\n",
        "                    val_feed_dict.update({self.is_training: False})\n",
        "\n",
        "                results = self.session.run(\n",
        "                    fetches=[self.loss] + list(self.metrics.values()),\n",
        "                    feed_dict=val_feed_dict\n",
        "                )\n",
        "                val_loss = results[0]\n",
        "                val_metrics = results[1:] if len(results) > 1 else []\n",
        "                val_metrics = dict(zip(self.metrics.keys(), val_metrics))\n",
        "                val_loss_history.append(val_loss)\n",
        "                val_time_history.append(time.time() - val_start)\n",
        "                for key in val_metrics:\n",
        "                    metric_histories[key].append(val_metrics[key])\n",
        "\n",
        "                if hasattr(self, 'monitor_tensors'):\n",
        "                    for name, tensor in self.monitor_tensors.items():\n",
        "                        [np_val] = self.session.run([tensor], feed_dict=val_feed_dict)\n",
        "                        print(name)\n",
        "                        print('min', np_val.min())\n",
        "                        print('max', np_val.max())\n",
        "                        print('mean', np_val.mean())\n",
        "                        print('std', np_val.std())\n",
        "                        print('nans', np.isnan(np_val).sum())\n",
        "                        print()\n",
        "                    print()\n",
        "                    print()\n",
        "\n",
        "                # train step\n",
        "                train_start = time.time()\n",
        "                train_batch_df = next(train_generator)\n",
        "                train_feed_dict = {\n",
        "                    getattr(self, placeholder_name, None): data\n",
        "                    for placeholder_name, data in train_batch_df.items() if hasattr(self, placeholder_name)\n",
        "                }\n",
        "\n",
        "                train_feed_dict.update(\n",
        "                    {self.learning_rate_var: self.learning_rate, self.beta1_decay_var: self.beta1_decay})\n",
        "                if hasattr(self, 'keep_prob'):\n",
        "                    train_feed_dict.update({self.keep_prob: self.keep_prob_scalar})\n",
        "                if hasattr(self, 'is_training'):\n",
        "                    train_feed_dict.update({self.is_training: True})\n",
        "\n",
        "                train_loss, _ = self.session.run(\n",
        "                    fetches=[self.loss, self.step],\n",
        "                    feed_dict=train_feed_dict\n",
        "                )\n",
        "                train_loss_history.append(train_loss)\n",
        "                train_time_history.append(time.time() - train_start)\n",
        "\n",
        "                if step % self.log_interval == 0:\n",
        "                    avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n",
        "                    avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n",
        "                    avg_train_time = sum(train_time_history) / len(train_time_history)\n",
        "                    avg_val_time = sum(val_time_history) / len(val_time_history)\n",
        "                    metric_log = (\n",
        "                        \"[[step {:>8}]]     \"\n",
        "                        \"[[train {:>4}s]]     loss: {:<12}     \"\n",
        "                        \"[[val {:>4}s]]     loss: {:<12}     \"\n",
        "                    ).format(\n",
        "                        step,\n",
        "                        round(avg_train_time, 4),\n",
        "                        round(avg_train_loss, 8),\n",
        "                        round(avg_val_time, 4),\n",
        "                        round(avg_val_loss, 8),\n",
        "                    )\n",
        "                    early_stopping_metric = avg_val_loss\n",
        "                    for metric_name, metric_history in metric_histories.items():\n",
        "                        metric_val = sum(metric_history) / len(metric_history)\n",
        "                        metric_log += '{}: {:<4}     '.format(metric_name, round(metric_val, 4))\n",
        "                        if metric_name == self.early_stopping_metric:\n",
        "                            early_stopping_metric = metric_val\n",
        "\n",
        "                    logging.info(metric_log)\n",
        "\n",
        "                    # Save the best step.\n",
        "                    if early_stopping_metric < best_validation_loss:\n",
        "                        logging.info('Updating best validation loss {} with early stopping metric {}.'.format(round(best_validation_loss,4),round(early_stopping_metric,4)))\n",
        "                        best_validation_loss = early_stopping_metric\n",
        "                        best_validation_tstep = step\n",
        "                        # Take a snapshot if the minimum number of steps have been reached.\n",
        "                        if step > self.min_steps_to_checkpoint:\n",
        "                            self.save(step)\n",
        "                            if self.enable_parameter_averaging:\n",
        "                                self.save(step, averaged=True)\n",
        "                            checkpoint_created=True\n",
        "\n",
        "                    # Stop training early and either restart with tigher training parameters or finish entirely.\n",
        "                    if step - best_validation_tstep > self.early_stopping_steps:\n",
        "                        logging.info('Stopping early at step {}: Best Validation Step: {} Early Stopping Steps: {}'.format(step, best_validation_tstep, self.early_stopping_steps))\n",
        "                        if self.num_restarts is None or self.restart_idx >= self.num_restarts:\n",
        "                            logging.info('Best validation loss of {} at training step {}'.format(best_validation_loss, best_validation_tstep))\n",
        "                            logging.info('Early stopping - ending training.')\n",
        "                            return\n",
        "\n",
        "                        #Restart the training with tighter parameters if we have remaining restarts and a checkpoint has been created.\n",
        "                        if self.restart_idx < self.num_restarts and checkpoint_created:\n",
        "                            logging.info('Restarting for the {} time out of {} total restarts.'.format(self.restart_idx, self.num_restarts))\n",
        "                            try:\n",
        "                                self.restore(best_validation_tstep)\n",
        "                            except Exception as error:\n",
        "                                logging.warn('Failed to restore checkpoint; will continue training: {} - {}'.format(type(error).__name__, error))\n",
        "                            else:\n",
        "                                step = best_validation_tstep\n",
        "                                self.restart_idx += 1\n",
        "                                self.update_train_params()\n",
        "                                train_generator = self.reader.train_batch_generator(self.batch_size)\n",
        "\n",
        "                step += 1\n",
        "\n",
        "            #Make sure at least one model gets saved.\n",
        "            if step <= self.min_steps_to_checkpoint:\n",
        "                # best_validation_tstep = step\n",
        "                self.save(step)\n",
        "                if self.enable_parameter_averaging:\n",
        "                    self.save(step, averaged=True)\n",
        "\n",
        "            logging.info('num_training_steps reached - ending training')\n",
        "\n",
        "    def predict(self, chunk_size=256):\n",
        "        if not os.path.isdir(self.prediction_dir):\n",
        "            os.makedirs(self.prediction_dir)\n",
        "\n",
        "        if hasattr(self, 'prediction_tensors'):\n",
        "            prediction_dict = {tensor_name: [] for tensor_name in self.prediction_tensors}\n",
        "\n",
        "            test_generator = self.reader.test_batch_generator(chunk_size)\n",
        "            for i, test_batch_df in enumerate(test_generator):\n",
        "                if i % 10 == 0:\n",
        "                    print(i * len(test_batch_df))\n",
        "\n",
        "                test_feed_dict = {\n",
        "                    getattr(self, placeholder_name, None): data\n",
        "                    for placeholder_name, data in test_batch_df.items() if hasattr(self, placeholder_name)\n",
        "                }\n",
        "                if hasattr(self, 'keep_prob'):\n",
        "                    test_feed_dict.update({self.keep_prob: 1.0})\n",
        "                if hasattr(self, 'is_training'):\n",
        "                    test_feed_dict.update({self.is_training: False})\n",
        "\n",
        "                tensor_names, tf_tensors = zip(*self.prediction_tensors.items())\n",
        "                np_tensors = self.session.run(\n",
        "                    fetches=tf_tensors,\n",
        "                    feed_dict=test_feed_dict\n",
        "                )\n",
        "                for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
        "                    prediction_dict[tensor_name].append(tensor)\n",
        "\n",
        "            for tensor_name, tensor in prediction_dict.items():\n",
        "                np_tensor = np.concatenate(tensor, 0)\n",
        "                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n",
        "                logging.info('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
        "                np.save(save_file, np_tensor)\n",
        "\n",
        "        if hasattr(self, 'parameter_tensors'):\n",
        "            for tensor_name, tensor in self.parameter_tensors.items():\n",
        "                np_tensor = tensor.eval(self.session)\n",
        "\n",
        "                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n",
        "                logging.info('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
        "                np.save(save_file, np_tensor)\n",
        "\n",
        "    def save(self, step, averaged=False):\n",
        "        saver = self.saver_averaged if averaged else self.saver\n",
        "        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n",
        "        if not os.path.isdir(checkpoint_dir):\n",
        "            logging.info('creating checkpoint directory {}'.format(checkpoint_dir))\n",
        "            os.mkdir(checkpoint_dir)\n",
        "\n",
        "        model_path = os.path.join(checkpoint_dir, 'model')\n",
        "        logging.info('saving model to {}'.format(model_path))\n",
        "        saver.save(self.session, model_path, global_step=step)\n",
        "\n",
        "    def restore(self, step=None, averaged=False):\n",
        "        saver = self.saver_averaged if averaged else self.saver\n",
        "        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n",
        "        if not step:\n",
        "            model_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "            logging.info('restoring model parameters from {}'.format(model_path))\n",
        "            saver.restore(self.session, model_path)\n",
        "        else:\n",
        "            model_path = os.path.join(\n",
        "                checkpoint_dir, 'model{}-{}'.format('_avg' if averaged else '', step)\n",
        "            )\n",
        "            logging.info('restoring model from {}'.format(model_path))\n",
        "            saver.restore(self.session, model_path)\n",
        "\n",
        "    def init_logging(self, log_dir):\n",
        "        if not os.path.isdir(log_dir):\n",
        "            os.makedirs(log_dir)\n",
        "\n",
        "        date_str = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
        "        log_file = 'log_{}.txt'.format(date_str)\n",
        "\n",
        "        import logging\n",
        "        logging.basicConfig(\n",
        "            filename=os.path.join(log_dir, log_file),\n",
        "            level=self.logging_level,\n",
        "            format='[[%(asctime)s]] %(message)s',\n",
        "            datefmt='%m/%d/%Y %I:%M:%S %p'\n",
        "        )\n",
        "        logging.getLogger().addHandler(logging.StreamHandler())\n",
        "\n",
        "    def update_parameters(self, loss):\n",
        "        if self.regularization_constant != 0:\n",
        "            l2_norm = tf.reduce_sum(\n",
        "                [tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tfcompat.trainable_variables()])\n",
        "            loss = loss + self.regularization_constant * l2_norm\n",
        "\n",
        "        optimizer = self.get_optimizer(self.learning_rate_var, self.beta1_decay_var)\n",
        "        grads = optimizer.compute_gradients(loss)\n",
        "        clipped = [(tf.clip_by_value(g, -self.grad_clip, self.grad_clip), v_) for g, v_ in grads]\n",
        "\n",
        "        update_ops = tfcompat.get_collection(tfcompat.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            step = optimizer.apply_gradients(clipped, global_step=self.global_step)\n",
        "\n",
        "        if self.enable_parameter_averaging:\n",
        "            maintain_averages_op = self.ema.apply(tfcompat.trainable_variables())\n",
        "            with tf.control_dependencies([step]):\n",
        "                self.step = tf.group(maintain_averages_op)\n",
        "        else:\n",
        "            self.step = step\n",
        "\n",
        "        logging.info('All parameters:')\n",
        "        logging.info(pp.pformat([(var.name, shape(var)) for var in tfcompat.global_variables()]))\n",
        "\n",
        "        logging.info('Trainable parameters:')\n",
        "        logging.info(pp.pformat([(var.name, shape(var)) for var in tfcompat.trainable_variables()]))\n",
        "\n",
        "        logging.info('Trainable parameter count:')\n",
        "        logging.info(str(np.sum(np.prod(shape(var)) for var in tfcompat.trainable_variables())))\n",
        "\n",
        "    def get_optimizer(self, learning_rate, beta1_decay):\n",
        "        if self.optimizer == 'adam':\n",
        "            return tfcompat.train.AdamOptimizer(learning_rate, beta1=beta1_decay)\n",
        "        elif self.optimizer == 'gd':\n",
        "            return tfcompat.train.GradientDescentOptimizer(learning_rate)\n",
        "        elif self.optimizer == 'rms':\n",
        "            return tfcompat.train.RMSPropOptimizer(learning_rate, decay=beta1_decay, momentum=0.9)\n",
        "        else:\n",
        "            assert False, 'Optimizer must be adam, gd, or rms'\n",
        "\n",
        "    def build_graph(self):\n",
        "        with tf.Graph().as_default() as graph:\n",
        "            self.ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
        "            self.global_step = tf.Variable(0, trainable=False)\n",
        "            self.learning_rate_var = tf.Variable(0.0, trainable=False)\n",
        "            self.beta1_decay_var = tf.Variable(0.0, trainable=False)\n",
        "\n",
        "            self.loss = self.calculate_loss()\n",
        "            self.update_parameters(self.loss)\n",
        "\n",
        "            self.saver = tfcompat.train.Saver(max_to_keep=1)\n",
        "            if self.enable_parameter_averaging:\n",
        "                self.saver_averaged = tfcompat.train.Saver(self.ema.variables_to_restore(), max_to_keep=1)\n",
        "\n",
        "            self.init = tfcompat.global_variables_initializer()\n",
        "            return graph"
      ],
      "metadata": {
        "id": "mCC6znEPcVU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = [\n",
        "    '\\x00', ' ', '!', '\"', '#', \"'\", '(', ')', ',', '-', '.',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';',\n",
        "    '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
        "    'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y',\n",
        "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "    'y', 'z'\n",
        "]\n",
        "alphabet_ord = list(map(ord, alphabet))\n",
        "alpha_to_num = defaultdict(int, list(map(reversed, enumerate(alphabet))))\n",
        "num_to_alpha = dict(enumerate(alphabet_ord))\n",
        "\n",
        "MAX_STROKE_LEN = 1200\n",
        "MAX_CHAR_LEN = 75\n",
        "\n",
        "\n",
        "def align(coords):\n",
        "    \"\"\"\n",
        "    corrects for global slant/offset in handwriting strokes\n",
        "    \"\"\"\n",
        "    coords = np.copy(coords)\n",
        "    x, y = coords[:, 0].reshape(-1, 1), coords[:, 1].reshape(-1, 1)\n",
        "    x = np.concatenate([np.ones([x.shape[0], 1]), x], axis=1)\n",
        "    offset, slope = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y).squeeze()\n",
        "    theta = np.arctan(slope)\n",
        "    rotation_matrix = np.array(\n",
        "        [[np.cos(theta), -np.sin(theta)],\n",
        "         [np.sin(theta), np.cos(theta)]]\n",
        "    )\n",
        "    coords[:, :2] = np.dot(coords[:, :2], rotation_matrix) - offset\n",
        "    return coords\n",
        "\n",
        "\n",
        "def skew(coords, degrees):\n",
        "    \"\"\"\n",
        "    skews strokes by given degrees\n",
        "    \"\"\"\n",
        "    coords = np.copy(coords)\n",
        "    theta = degrees * np.pi / 180\n",
        "    a = np.array([[np.cos(-theta), 0], [np.sin(-theta), 1]])\n",
        "    coords[:, :2] = np.dot(coords[:, :2], a)\n",
        "    return coords\n",
        "\n",
        "\n",
        "def stretch(coords, x_factor, y_factor):\n",
        "    \"\"\"\n",
        "    stretches strokes along x and y-axis\n",
        "    \"\"\"\n",
        "    coords = np.copy(coords)\n",
        "    coords[:, :2] *= np.array([x_factor, y_factor])\n",
        "    return coords\n",
        "\n",
        "\n",
        "def add_noise(coords, scale):\n",
        "    \"\"\"\n",
        "    adds gaussian noise to strokes\n",
        "    \"\"\"\n",
        "    coords = np.copy(coords)\n",
        "    coords[1:, :2] += np.random.normal(loc=0.0, scale=scale, size=coords[1:, :2].shape)\n",
        "    return coords\n",
        "\n",
        "\n",
        "def encode_ascii(ascii_string):\n",
        "    \"\"\"\n",
        "    encodes ascii string to array of ints\n",
        "    \"\"\"\n",
        "    return np.array(list(map(lambda x: alpha_to_num[x], ascii_string)) + [0])\n",
        "\n",
        "\n",
        "def denoise(coords):\n",
        "    \"\"\"\n",
        "    smoothing filter to mitigate some artifacts of the data collection\n",
        "    \"\"\"\n",
        "    coords = np.split(coords, np.where(coords[:, 2] == 1)[0] + 1, axis=0)\n",
        "    new_coords = []\n",
        "    for stroke in coords:\n",
        "        if len(stroke) != 0:\n",
        "            x_new = savgol_filter(stroke[:, 0], 7, 3, mode='nearest')\n",
        "            y_new = savgol_filter(stroke[:, 1], 7, 3, mode='nearest')\n",
        "            xy_coords = np.hstack([x_new.reshape(-1, 1), y_new.reshape(-1, 1)])\n",
        "            stroke = np.concatenate([xy_coords, stroke[:, 2].reshape(-1, 1)], axis=1)\n",
        "            new_coords.append(stroke)\n",
        "\n",
        "    coords = np.vstack(new_coords)\n",
        "    return coords\n",
        "\n",
        "\n",
        "def interpolate(coords, factor=2):\n",
        "    \"\"\"\n",
        "    interpolates strokes using cubic spline\n",
        "    \"\"\"\n",
        "    coords = np.split(coords, np.where(coords[:, 2] == 1)[0] + 1, axis=0)\n",
        "    new_coords = []\n",
        "    for stroke in coords:\n",
        "\n",
        "        if len(stroke) == 0:\n",
        "            continue\n",
        "\n",
        "        xy_coords = stroke[:, :2]\n",
        "\n",
        "        if len(stroke) > 3:\n",
        "            f_x = interp1d(np.arange(len(stroke)), stroke[:, 0], kind='cubic')\n",
        "            f_y = interp1d(np.arange(len(stroke)), stroke[:, 1], kind='cubic')\n",
        "\n",
        "            xx = np.linspace(0, len(stroke) - 1, factor * (len(stroke)))\n",
        "            yy = np.linspace(0, len(stroke) - 1, factor * (len(stroke)))\n",
        "\n",
        "            x_new = f_x(xx)\n",
        "            y_new = f_y(yy)\n",
        "\n",
        "            xy_coords = np.hstack([x_new.reshape(-1, 1), y_new.reshape(-1, 1)])\n",
        "\n",
        "        stroke_eos = np.zeros([len(xy_coords), 1])\n",
        "        stroke_eos[-1] = 1.0\n",
        "        stroke = np.concatenate([xy_coords, stroke_eos], axis=1)\n",
        "        new_coords.append(stroke)\n",
        "\n",
        "    coords = np.vstack(new_coords)\n",
        "    return coords\n",
        "\n",
        "\n",
        "def normalize(offsets):\n",
        "    \"\"\"\n",
        "    normalizes strokes to median unit norm\n",
        "    \"\"\"\n",
        "    offsets = np.copy(offsets)\n",
        "    offsets[:, :2] /= np.median(np.linalg.norm(offsets[:, :2], axis=1))\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def coords_to_offsets(coords):\n",
        "    \"\"\"\n",
        "    convert from coordinates to offsets\n",
        "    \"\"\"\n",
        "    offsets = np.concatenate([coords[1:, :2] - coords[:-1, :2], coords[1:, 2:3]], axis=1)\n",
        "    offsets = np.concatenate([np.array([[0, 0, 1]]), offsets], axis=0)\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def offsets_to_coords(offsets):\n",
        "    \"\"\"\n",
        "    convert from offsets to coordinates\n",
        "    \"\"\"\n",
        "    return np.concatenate([np.cumsum(offsets[:, :2], axis=0), offsets[:, 2:3]], axis=1)\n",
        "\n",
        "\n",
        "def draw(\n",
        "        offsets,\n",
        "        ascii_seq=None,\n",
        "        align_strokes=True,\n",
        "        denoise_strokes=True,\n",
        "        interpolation_factor=None,\n",
        "        save_file=None\n",
        "):\n",
        "    strokes = offsets_to_coords(offsets)\n",
        "\n",
        "    if denoise_strokes:\n",
        "        strokes = denoise(strokes)\n",
        "\n",
        "    if interpolation_factor is not None:\n",
        "        strokes = interpolate(strokes, factor=interpolation_factor)\n",
        "\n",
        "    if align_strokes:\n",
        "        strokes[:, :2] = align(strokes[:, :2])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 3))\n",
        "\n",
        "    stroke = []\n",
        "    for x, y, eos in strokes:\n",
        "        stroke.append((x, y))\n",
        "        if eos == 1:\n",
        "            coords = zip(*stroke)\n",
        "            ax.plot(coords[0], coords[1], 'k')\n",
        "            stroke = []\n",
        "    if stroke:\n",
        "        coords = zip(*stroke)\n",
        "        ax.plot(coords[0], coords[1], 'k')\n",
        "        stroke = []\n",
        "\n",
        "    ax.set_xlim(-50, 600)\n",
        "    ax.set_ylim(-40, 40)\n",
        "\n",
        "    ax.set_aspect('equal')\n",
        "    plt.tick_params(\n",
        "        axis='both',\n",
        "        left='off',\n",
        "        top='off',\n",
        "        right='off',\n",
        "        bottom='off',\n",
        "        labelleft='off',\n",
        "        labeltop='off',\n",
        "        labelright='off',\n",
        "        labelbottom='off'\n",
        "    )\n",
        "\n",
        "    if ascii_seq is not None:\n",
        "        if not isinstance(ascii_seq, str):\n",
        "            ascii_seq = ''.join(list(map(chr, ascii_seq)))\n",
        "        plt.title(ascii_seq)\n",
        "\n",
        "    if save_file is not None:\n",
        "        plt.savefig(save_file)\n",
        "        print('saved to {}'.format(save_file))\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close('all')"
      ],
      "metadata": {
        "id": "CMAijI7GeYxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "CgCKmvMtm86v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework import constant_op\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import tensor_array_ops\n",
        "from tensorflow.python.ops import variable_scope as vs\n",
        "from tensorflow.python.ops.rnn import _maybe_tensor_shape_from_tensor\n",
        "from tensorflow.python.ops.rnn_cell_impl import _concat, assert_like_rnncell\n",
        "from tensorflow.python.util import is_in_graph_mode\n",
        "from tensorflow.python.util import nest\n",
        "\n",
        "\n",
        "def raw_rnn(cell, loop_fn, parallel_iterations=None, swap_memory=False, scope=None):\n",
        "    \"\"\"\n",
        "    raw_rnn adapted from the original tensorflow implementation\n",
        "    (https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/rnn.py)\n",
        "    to emit arbitrarily nested states for each time step (concatenated along the time axis)\n",
        "    in addition to the outputs at each timestep and the final state\n",
        "\n",
        "    returns (\n",
        "        states for all timesteps,\n",
        "        outputs for all timesteps,\n",
        "        final cell state,\n",
        "    )\n",
        "    \"\"\"\n",
        "    assert_like_rnncell(\"Raw rnn cell\", cell)\n",
        "\n",
        "    if not callable(loop_fn):\n",
        "        raise TypeError(\"loop_fn must be a callable\")\n",
        "\n",
        "    parallel_iterations = parallel_iterations or 32\n",
        "\n",
        "    # Create a new scope in which the caching device is either\n",
        "    # determined by the parent scope, or is set to place the cached\n",
        "    # Variable using the same placement as for the rest of the RNN.\n",
        "    with vs.variable_scope(scope or \"rnn\") as varscope:\n",
        "        if is_in_graph_mode.IS_IN_GRAPH_MODE():\n",
        "            if varscope.caching_device is None:\n",
        "                varscope.set_caching_device(lambda op: op.device)\n",
        "\n",
        "        time = constant_op.constant(0, dtype=dtypes.int32)\n",
        "        (elements_finished, next_input,\n",
        "         initial_state, emit_structure, init_loop_state) = loop_fn(\n",
        "            time, None, None, None)  # time, cell_output, cell_state, loop_state\n",
        "        flat_input = nest.flatten(next_input)\n",
        "\n",
        "        # Need a surrogate loop state for the while_loop if none is available.\n",
        "        loop_state = (\n",
        "            init_loop_state if init_loop_state is not None else\n",
        "            constant_op.constant(0, dtype=dtypes.int32))\n",
        "\n",
        "        input_shape = [input_.get_shape() for input_ in flat_input]\n",
        "        static_batch_size = tensor_shape.dimension_at_index(input_shape[0], 0)\n",
        "\n",
        "        for input_shape_i in input_shape:\n",
        "            # Static verification that batch sizes all match\n",
        "            static_batch_size.assert_is_compatible_with(\n",
        "                tensor_shape.dimension_at_index(input_shape_i, 0))\n",
        "\n",
        "        batch_size = tensor_shape.dimension_value(static_batch_size)\n",
        "        const_batch_size = batch_size\n",
        "        if batch_size is None:\n",
        "            batch_size = array_ops.shape(flat_input[0])[0]\n",
        "\n",
        "        nest.assert_same_structure(initial_state, cell.state_size)\n",
        "        state = initial_state\n",
        "        flat_state = nest.flatten(state)\n",
        "        flat_state = [ops.convert_to_tensor(s) for s in flat_state]\n",
        "        state = nest.pack_sequence_as(structure=state, flat_sequence=flat_state)\n",
        "\n",
        "        if emit_structure is not None:\n",
        "            flat_emit_structure = nest.flatten(emit_structure)\n",
        "            flat_emit_size = [emit.shape if emit.shape.is_fully_defined() else\n",
        "                              array_ops.shape(emit) for emit in flat_emit_structure]\n",
        "            flat_emit_dtypes = [emit.dtype for emit in flat_emit_structure]\n",
        "        else:\n",
        "            emit_structure = cell.output_size\n",
        "            flat_emit_size = nest.flatten(emit_structure)\n",
        "            flat_emit_dtypes = [flat_state[0].dtype] * len(flat_emit_size)\n",
        "\n",
        "        flat_state_size = [s.shape if s.shape.is_fully_defined() else\n",
        "                           array_ops.shape(s) for s in flat_state]\n",
        "        flat_state_dtypes = [s.dtype for s in flat_state]\n",
        "\n",
        "        flat_emit_ta = [\n",
        "            tensor_array_ops.TensorArray(\n",
        "                dtype=dtype_i,\n",
        "                dynamic_size=True,\n",
        "                element_shape=(tensor_shape.TensorShape([const_batch_size])\n",
        "                               .concatenate(_maybe_tensor_shape_from_tensor(size_i))),\n",
        "                size=0,\n",
        "                name=\"rnn_output_%d\" % i\n",
        "            )\n",
        "            for i, (dtype_i, size_i) in enumerate(zip(flat_emit_dtypes, flat_emit_size))\n",
        "        ]\n",
        "        emit_ta = nest.pack_sequence_as(structure=emit_structure, flat_sequence=flat_emit_ta)\n",
        "        flat_zero_emit = [\n",
        "            array_ops.zeros(_concat(batch_size, size_i), dtype_i)\n",
        "            for size_i, dtype_i in zip(flat_emit_size, flat_emit_dtypes)]\n",
        "\n",
        "        zero_emit = nest.pack_sequence_as(structure=emit_structure, flat_sequence=flat_zero_emit)\n",
        "\n",
        "        flat_state_ta = [\n",
        "            tensor_array_ops.TensorArray(\n",
        "                dtype=dtype_i,\n",
        "                dynamic_size=True,\n",
        "                element_shape=(tensor_shape.TensorShape([const_batch_size])\n",
        "                               .concatenate(_maybe_tensor_shape_from_tensor(size_i))),\n",
        "                size=0,\n",
        "                name=\"rnn_state_%d\" % i\n",
        "            )\n",
        "            for i, (dtype_i, size_i) in enumerate(zip(flat_state_dtypes, flat_state_size))\n",
        "        ]\n",
        "        state_ta = nest.pack_sequence_as(structure=state, flat_sequence=flat_state_ta)\n",
        "\n",
        "        def condition(unused_time, elements_finished, *_):\n",
        "            return math_ops.logical_not(math_ops.reduce_all(elements_finished))\n",
        "\n",
        "        def body(time, elements_finished, current_input, state_ta, emit_ta, state, loop_state):\n",
        "            (next_output, cell_state) = cell(current_input, state)\n",
        "\n",
        "            nest.assert_same_structure(state, cell_state)\n",
        "            nest.assert_same_structure(cell.output_size, next_output)\n",
        "\n",
        "            next_time = time + 1\n",
        "            (next_finished, next_input, next_state, emit_output,\n",
        "             next_loop_state) = loop_fn(next_time, next_output, cell_state, loop_state)\n",
        "\n",
        "            nest.assert_same_structure(state, next_state)\n",
        "            nest.assert_same_structure(current_input, next_input)\n",
        "            nest.assert_same_structure(emit_ta, emit_output)\n",
        "\n",
        "            # If loop_fn returns None for next_loop_state, just reuse the previous one.\n",
        "            loop_state = loop_state if next_loop_state is None else next_loop_state\n",
        "\n",
        "            def _copy_some_through(current, candidate):\n",
        "                \"\"\"Copy some tensors through via array_ops.where.\"\"\"\n",
        "\n",
        "                def copy_fn(cur_i, cand_i):\n",
        "                    # TensorArray and scalar get passed through.\n",
        "                    if isinstance(cur_i, tensor_array_ops.TensorArray):\n",
        "                        return cand_i\n",
        "                    if cur_i.shape.ndims == 0:\n",
        "                        return cand_i\n",
        "                    # Otherwise propagate the old or the new value.\n",
        "                    with ops.colocate_with(cand_i):\n",
        "                        return array_ops.where(elements_finished, cur_i, cand_i)\n",
        "\n",
        "                return nest.map_structure(copy_fn, current, candidate)\n",
        "\n",
        "            emit_output = _copy_some_through(zero_emit, emit_output)\n",
        "            next_state = _copy_some_through(state, next_state)\n",
        "\n",
        "            emit_ta = nest.map_structure(lambda ta, emit: ta.write(time, emit), emit_ta, emit_output)\n",
        "            state_ta = nest.map_structure(lambda ta, state: ta.write(time, state), state_ta, next_state)\n",
        "\n",
        "            elements_finished = math_ops.logical_or(elements_finished, next_finished)\n",
        "\n",
        "            return (next_time, elements_finished, next_input, state_ta,\n",
        "                    emit_ta, next_state, loop_state)\n",
        "\n",
        "        returned = tf.while_loop(\n",
        "            condition, body, loop_vars=[\n",
        "                time, elements_finished, next_input, state_ta,\n",
        "                emit_ta, state, loop_state],\n",
        "            parallel_iterations=parallel_iterations,\n",
        "            swap_memory=swap_memory\n",
        "        )\n",
        "\n",
        "        (state_ta, emit_ta, final_state, final_loop_state) = returned[-4:]\n",
        "\n",
        "        flat_states = nest.flatten(state_ta)\n",
        "        flat_states = [array_ops.transpose(ta.stack(), (1, 0, 2)) for ta in flat_states]\n",
        "        states = nest.pack_sequence_as(structure=state_ta, flat_sequence=flat_states)\n",
        "\n",
        "        flat_outputs = nest.flatten(emit_ta)\n",
        "        flat_outputs = [array_ops.transpose(ta.stack(), (1, 0, 2)) for ta in flat_outputs]\n",
        "        outputs = nest.pack_sequence_as(structure=emit_ta, flat_sequence=flat_outputs)\n",
        "\n",
        "        return (states, outputs, final_state)\n",
        "\n",
        "\n",
        "def rnn_teacher_force(inputs, cell, sequence_length, initial_state, scope='dynamic-rnn-teacher-force'):\n",
        "    \"\"\"\n",
        "    Implementation of an rnn with teacher forcing inputs provided.\n",
        "    Used in the same way as tf.dynamic_rnn.\n",
        "    \"\"\"\n",
        "    inputs = array_ops.transpose(inputs, (1, 0, 2))\n",
        "    inputs_ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n",
        "    inputs_ta = inputs_ta.unstack(inputs)\n",
        "\n",
        "    def loop_fn(time, cell_output, cell_state, loop_state):\n",
        "        emit_output = cell_output\n",
        "        next_cell_state = initial_state if cell_output is None else cell_state\n",
        "\n",
        "        elements_finished = time >= sequence_length\n",
        "        finished = math_ops.reduce_all(elements_finished)\n",
        "\n",
        "        next_input = tf.cond(\n",
        "            finished,\n",
        "            lambda: array_ops.zeros([array_ops.shape(inputs)[1], inputs.shape.as_list()[2]], dtype=dtypes.float32),\n",
        "            lambda: inputs_ta.read(time)\n",
        "        )\n",
        "\n",
        "        next_loop_state = None\n",
        "        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
        "\n",
        "    states, outputs, final_state = raw_rnn(cell, loop_fn, scope=scope)\n",
        "    return states, outputs, final_state\n",
        "\n",
        "\n",
        "def rnn_free_run(cell, initial_state, sequence_length, initial_input=None, scope='dynamic-rnn-free-run'):\n",
        "    \"\"\"\n",
        "    Implementation of an rnn which feeds its feeds its predictions back to itself at the next timestep.\n",
        "\n",
        "    cell must implement two methods:\n",
        "\n",
        "        cell.output_function(state) which takes in the state at timestep t and returns\n",
        "        the cell input at timestep t+1.\n",
        "\n",
        "        cell.termination_condition(state) which returns a boolean tensor of shape\n",
        "        [batch_size] denoting which sequences no longer need to be sampled.\n",
        "    \"\"\"\n",
        "    with vs.variable_scope(scope, reuse=True):\n",
        "        if initial_input is None:\n",
        "            initial_input = cell.output_function(initial_state)\n",
        "\n",
        "    def loop_fn(time, cell_output, cell_state, loop_state):\n",
        "        next_cell_state = initial_state if cell_output is None else cell_state\n",
        "\n",
        "        elements_finished = math_ops.logical_or(\n",
        "            time >= sequence_length,\n",
        "            cell.termination_condition(next_cell_state)\n",
        "        )\n",
        "        finished = math_ops.reduce_all(elements_finished)\n",
        "\n",
        "        next_input = tf.cond(\n",
        "            finished,\n",
        "            lambda: array_ops.zeros_like(initial_input),\n",
        "            lambda: initial_input if cell_output is None else cell.output_function(next_cell_state)\n",
        "        )\n",
        "        emit_output = next_input[0] if cell_output is None else next_input\n",
        "\n",
        "        next_loop_state = None\n",
        "        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
        "\n",
        "    states, outputs, final_state = raw_rnn(cell, loop_fn, scope=scope)\n",
        "    return states, outputs, final_state"
      ],
      "metadata": {
        "id": "LXdffVR3fAeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(BaseModel):\n",
        "    def __init__(\n",
        "            self,\n",
        "            lstm_size,\n",
        "            output_mixture_components,\n",
        "            attention_mixture_components,\n",
        "            **kwargs\n",
        "    ):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "        self.x_len = None\n",
        "        self.c = None\n",
        "        self.c_len = None\n",
        "        self.sample_tsteps = None\n",
        "        self.num_samples = None\n",
        "        self.prime = None\n",
        "        self.x_prime = None\n",
        "        self.x_prime_len = None\n",
        "        self.bias = None\n",
        "        self.initial_state = None\n",
        "        self.final_state = None\n",
        "        self.sampled_sequence = None\n",
        "        self.lstm_size = lstm_size\n",
        "        self.output_mixture_components = output_mixture_components\n",
        "        self.output_units = self.output_mixture_components * 6 + 1\n",
        "        self.attention_mixture_components = attention_mixture_components\n",
        "        super(RNN, self).__init__(**kwargs)\n",
        "\n",
        "    def parse_parameters(self, z, eps=1e-8, sigma_eps=1e-4):\n",
        "        pis, sigmas, rhos, mus, es = tf.split(\n",
        "            z,\n",
        "            [\n",
        "                1 * self.output_mixture_components,\n",
        "                2 * self.output_mixture_components,\n",
        "                1 * self.output_mixture_components,\n",
        "                2 * self.output_mixture_components,\n",
        "                1\n",
        "            ],\n",
        "            axis=-1\n",
        "        )\n",
        "        pis = tf.nn.softmax(pis, axis=-1)\n",
        "        sigmas = tf.clip_by_value(tf.exp(sigmas), sigma_eps, np.inf)\n",
        "        rhos = tf.clip_by_value(tf.tanh(rhos), eps - 1.0, 1.0 - eps)\n",
        "        es = tf.clip_by_value(tf.nn.sigmoid(es), eps, 1.0 - eps)\n",
        "        return pis, mus, sigmas, rhos, es\n",
        "\n",
        "    @staticmethod\n",
        "    def nll(y, lengths, pis, mus, sigmas, rho, es, eps=1e-8):\n",
        "        sigma_1, sigma_2 = tf.split(sigmas, 2, axis=2)\n",
        "        y_1, y_2, y_3 = tf.split(y, 3, axis=2)\n",
        "        mu_1, mu_2 = tf.split(mus, 2, axis=2)\n",
        "\n",
        "        norm = 1.0 / (2 * np.pi * sigma_1 * sigma_2 * tf.sqrt(1 - tf.square(rho)))\n",
        "        z = tf.square((y_1 - mu_1) / sigma_1) + \\\n",
        "            tf.square((y_2 - mu_2) / sigma_2) - \\\n",
        "            2 * rho * (y_1 - mu_1) * (y_2 - mu_2) / (sigma_1 * sigma_2)\n",
        "\n",
        "        exp = -1.0 * z / (2 * (1 - tf.square(rho)))\n",
        "        gaussian_likelihoods = tf.exp(exp) * norm\n",
        "        gmm_likelihood = tf.reduce_sum(pis * gaussian_likelihoods, 2)\n",
        "        gmm_likelihood = tf.clip_by_value(gmm_likelihood, eps, np.inf)\n",
        "\n",
        "        bernoulli_likelihood = tf.squeeze(tfcompat.where(tf.equal(tf.ones_like(y_3), y_3), es, 1 - es))\n",
        "\n",
        "        nll = -(tf.math.log(gmm_likelihood) + tf.math.log(bernoulli_likelihood))\n",
        "        sequence_mask = tf.logical_and(\n",
        "            tf.sequence_mask(lengths, maxlen=tf.shape(y)[1]),\n",
        "            tf.logical_not(tf.math.is_nan(nll)),\n",
        "        )\n",
        "        nll = tfcompat.where(sequence_mask, nll, tf.zeros_like(nll))\n",
        "        num_valid = tf.reduce_sum(tf.cast(sequence_mask, tf.float32), axis=1)\n",
        "\n",
        "        sequence_loss = tf.reduce_sum(nll, axis=1) / tf.maximum(num_valid, 1.0)\n",
        "        element_loss = tf.reduce_sum(nll) / tf.maximum(tf.reduce_sum(num_valid), 1.0)\n",
        "        return sequence_loss, element_loss\n",
        "\n",
        "    def sample(self, cell):\n",
        "        initial_state = cell.zero_state(self.num_samples, dtype=tf.float32)\n",
        "        initial_input = tf.concat([\n",
        "            tf.zeros([self.num_samples, 2]),\n",
        "            tf.ones([self.num_samples, 1]),\n",
        "        ], axis=1)\n",
        "        return rnn_free_run(\n",
        "            cell=cell,\n",
        "            sequence_length=self.sample_tsteps,\n",
        "            initial_state=initial_state,\n",
        "            initial_input=initial_input,\n",
        "            scope='rnn'\n",
        "        )[1]\n",
        "\n",
        "    def primed_sample(self, cell):\n",
        "        initial_state = cell.zero_state(self.num_samples, dtype=tf.float32)\n",
        "        primed_state = tfcompat.nn.dynamic_rnn(\n",
        "            inputs=self.x_prime,\n",
        "            cell=cell,\n",
        "            sequence_length=self.x_prime_len,\n",
        "            dtype=tf.float32,\n",
        "            initial_state=initial_state,\n",
        "            scope='rnn'\n",
        "        )[1]\n",
        "        return rnn_free_run(\n",
        "            cell=cell,\n",
        "            sequence_length=self.sample_tsteps,\n",
        "            initial_state=primed_state,\n",
        "            scope='rnn'\n",
        "        )[1]\n",
        "\n",
        "    def calculate_loss(self):\n",
        "        self.x = tfcompat.placeholder(tf.float32, [None, None, 3])\n",
        "        self.y = tfcompat.placeholder(tf.float32, [None, None, 3])\n",
        "        self.x_len = tfcompat.placeholder(tf.int32, [None])\n",
        "        self.c = tfcompat.placeholder(tf.int32, [None, None])\n",
        "        self.c_len = tfcompat.placeholder(tf.int32, [None])\n",
        "\n",
        "        self.sample_tsteps = tfcompat.placeholder(tf.int32, [])\n",
        "        self.num_samples = tfcompat.placeholder(tf.int32, [])\n",
        "        self.prime = tfcompat.placeholder(tf.bool, [])\n",
        "        self.x_prime = tfcompat.placeholder(tf.float32, [None, None, 3])\n",
        "        self.x_prime_len = tfcompat.placeholder(tf.int32, [None])\n",
        "        self.bias = tfcompat.placeholder_with_default(\n",
        "            tf.zeros([self.num_samples], dtype=tf.float32), [None])\n",
        "\n",
        "        cell = LSTMAttentionCell(\n",
        "            lstm_size=self.lstm_size,\n",
        "            num_attn_mixture_components=self.attention_mixture_components,\n",
        "            attention_values=tf.one_hot(self.c, len(alphabet)),\n",
        "            attention_values_lengths=self.c_len,\n",
        "            num_output_mixture_components=self.output_mixture_components,\n",
        "            bias=self.bias\n",
        "        )\n",
        "        self.initial_state = cell.zero_state(tf.shape(self.x)[0], dtype=tf.float32)\n",
        "        outputs, self.final_state = tfcompat.nn.dynamic_rnn(\n",
        "            inputs=self.x,\n",
        "            cell=cell,\n",
        "            sequence_length=self.x_len,\n",
        "            dtype=tf.float32,\n",
        "            initial_state=self.initial_state,\n",
        "            scope='rnn'\n",
        "        )\n",
        "        params = time_distributed_dense_layer(outputs, self.output_units, scope='rnn/gmm')\n",
        "        pis, mus, sigmas, rhos, es = self.parse_parameters(params)\n",
        "        sequence_loss, self.loss = self.nll(self.y, self.x_len, pis, mus, sigmas, rhos, es)\n",
        "\n",
        "        self.sampled_sequence = tf.cond(\n",
        "            self.prime,\n",
        "            lambda: self.primed_sample(cell),\n",
        "            lambda: self.sample(cell)\n",
        "        )\n",
        "        return self.loss"
      ],
      "metadata": {
        "id": "lJFkfFebfOu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_generator(batch_size, df, shuffle=True, num_epochs=10000, mode='train'):\n",
        "    gen = df.batch_generator(\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_epochs=num_epochs,\n",
        "        allow_smaller_final_batch=(mode == 'test')\n",
        "    )\n",
        "    for batch in gen:\n",
        "        batch['x_len'] = batch['x_len'] - 1\n",
        "        max_x_len = np.max(batch['x_len'])\n",
        "        max_c_len = np.max(batch['c_len'])\n",
        "        batch['y'] = batch['x'][:, 1:max_x_len + 1, :]\n",
        "        batch['x'] = batch['x'][:, :max_x_len, :]\n",
        "        batch['c'] = batch['c'][:, :max_c_len]\n",
        "        yield batch"
      ],
      "metadata": {
        "id": "nhPzPjZRfc5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataReader(object):\n",
        "    def __init__(self, data_dir):\n",
        "        data_cols = ['x', 'x_len', 'c', 'c_len']\n",
        "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i))) for i in data_cols]\n",
        "\n",
        "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
        "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.95, random_state=2018)\n",
        "\n",
        "        print('train size', len(self.train_df))\n",
        "        print('val size', len(self.val_df))\n",
        "        print('test size', len(self.test_df))\n",
        "\n",
        "    def train_batch_generator(self, batch_size):\n",
        "        return batch_generator(\n",
        "            batch_size=batch_size,\n",
        "            df=self.train_df,\n",
        "            shuffle=True,\n",
        "            num_epochs=10000,\n",
        "            mode='train'\n",
        "        )\n",
        "\n",
        "    def val_batch_generator(self, batch_size):\n",
        "        return batch_generator(\n",
        "            batch_size=batch_size,\n",
        "            df=self.val_df,\n",
        "            shuffle=True,\n",
        "            num_epochs=10000,\n",
        "            mode='val'\n",
        "        )\n",
        "\n",
        "    def test_batch_generator(self, batch_size):\n",
        "        return batch_generator(\n",
        "            batch_size=batch_size,\n",
        "            df=self.test_df,\n",
        "            shuffle=False,\n",
        "            num_epochs=1,\n",
        "            mode='test'\n",
        "        )"
      ],
      "metadata": {
        "id": "R_FPEHwHfiuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    dr = DataReader(data_dir=processed_data_path)\n",
        "\n",
        "    nn = RNN(\n",
        "        reader=dr,\n",
        "        log_dir='logs',\n",
        "        checkpoint_dir=checkpoint_path,\n",
        "        prediction_dir=prediction_path,\n",
        "        learning_rates=[.0001, .00005, .00002],\n",
        "        batch_sizes=[32, 64, 64],\n",
        "        patiences=[1500, 1000, 500],\n",
        "        beta1_decays=[.9, .9, .9],\n",
        "        validation_batch_size=32,\n",
        "        optimizer='rms',\n",
        "        num_training_steps=100000,\n",
        "        warm_start_init_step=0,\n",
        "        regularization_constant=0.0,\n",
        "        keep_prob=1.0,\n",
        "        enable_parameter_averaging=False,\n",
        "        min_steps_to_checkpoint=2000,\n",
        "        log_interval=20,\n",
        "        grad_clip=10,\n",
        "        lstm_size=400,\n",
        "        output_mixture_components=20,\n",
        "        attention_mixture_components=10\n",
        "    )\n",
        "    nn.fit()\n"
      ],
      "metadata": {
        "id": "uJ58Eygofs5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "AyjilcOxf0H9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}